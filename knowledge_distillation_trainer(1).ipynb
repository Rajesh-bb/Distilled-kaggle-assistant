{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:25:42.542478Z",
     "iopub.status.busy": "2026-01-22T16:25:42.542208Z",
     "iopub.status.idle": "2026-01-22T16:26:02.824710Z",
     "shell.execute_reply": "2026-01-22T16:26:02.823535Z",
     "shell.execute_reply.started": "2026-01-22T16:25:42.542443Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (26.0rc2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.49.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -q peft\n",
    "!pip install -q accelerate\n",
    "!pip install -U bitsandbytes\n",
    "!pip install -q transformers\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:26:36.803951Z",
     "iopub.status.busy": "2026-01-22T16:26:36.803356Z",
     "iopub.status.idle": "2026-01-22T16:26:42.574526Z",
     "shell.execute_reply": "2026-01-22T16:26:42.573760Z",
     "shell.execute_reply.started": "2026-01-22T16:26:36.803913Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting GPUtil\n",
      "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: GPUtil\n",
      "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=c146cd718762ea5737f85fad55eb72bc0bebd25d64bee23b3bd63961ea913008\n",
      "  Stored in directory: /root/.cache/pip/wheels/92/a8/b7/d8a067c31a74de9ca252bbe53dea5f896faabd25d55f541037\n",
      "Successfully built GPUtil\n",
      "Installing collected packages: GPUtil\n",
      "Successfully installed GPUtil-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:26:46.199453Z",
     "iopub.status.busy": "2026-01-22T16:26:46.199158Z",
     "iopub.status.idle": "2026-01-22T16:26:49.508504Z",
     "shell.execute_reply": "2026-01-22T16:26:49.507691Z",
     "shell.execute_reply.started": "2026-01-22T16:26:46.199426Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% |  0% |\n",
      "|  1 |  0% |  0% |\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import GPUtil\n",
    "import os\n",
    "\n",
    "GPUtil.showUtilization()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is not available, using CPU instead\")\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:27:24.163021Z",
     "iopub.status.busy": "2026-01-22T16:27:24.162242Z",
     "iopub.status.idle": "2026-01-22T16:27:46.524975Z",
     "shell.execute_reply": "2026-01-22T16:27:46.524204Z",
     "shell.execute_reply.started": "2026-01-22T16:27:24.162990Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 16:27:33.657523: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1769099253.806480      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1769099253.851996      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1769099254.205038      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769099254.205075      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769099254.205078      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769099254.205081      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, LlamaTokenizer\n",
    "from huggingface_hub import notebook_login\n",
    "#from datasets import load_dataset\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "if \"COLAB_GPU\" in os.environ:\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:30:41.789439Z",
     "iopub.status.busy": "2026-01-22T16:30:41.788650Z",
     "iopub.status.idle": "2026-01-22T16:30:41.806416Z",
     "shell.execute_reply": "2026-01-22T16:30:41.805483Z",
     "shell.execute_reply.started": "2026-01-22T16:30:41.789404Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b150869297184709892aace130de0160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if \"COLAB_GPU\" in os.environ:\n",
    "  !huggingface-cli login\n",
    "else:\n",
    "  notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T20:53:19.720351Z",
     "iopub.status.busy": "2026-01-21T20:53:19.720037Z",
     "iopub.status.idle": "2026-01-21T20:54:12.199465Z",
     "shell.execute_reply": "2026-01-21T20:54:12.198916Z",
     "shell.execute_reply.started": "2026-01-21T20:53:19.720324Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a17ba9abb94d098f07327ea7056c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb9b8c140c93425ba2e095f735d176d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d792e50821324ed7b68381f4a32a01bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04991b6f831543d18689b9212c27fafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6634230cfe042b3a1062dc77860387a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfb657e4b264e9f9afd336d23169b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3501f024c127413fb663ae64eaab415f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config,device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T20:54:46.583470Z",
     "iopub.status.busy": "2026-01-21T20:54:46.582753Z",
     "iopub.status.idle": "2026-01-21T20:54:47.749164Z",
     "shell.execute_reply": "2026-01-21T20:54:47.748586Z",
     "shell.execute_reply.started": "2026-01-21T20:54:46.583437Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T20:54:53.209106Z",
     "iopub.status.busy": "2026-01-21T20:54:53.208811Z",
     "iopub.status.idle": "2026-01-21T20:54:55.004490Z",
     "shell.execute_reply": "2026-01-21T20:54:55.003719Z",
     "shell.execute_reply.started": "2026-01-21T20:54:53.209079Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8c5cecefbb4959bcfaa7260d111017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccda67780a3742e4b756452475bb5781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ac6caaabfa4a8d89070c1141385c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:34:17.953472Z",
     "iopub.status.busy": "2026-01-22T16:34:17.952908Z",
     "iopub.status.idle": "2026-01-22T16:34:17.957425Z",
     "shell.execute_reply": "2026-01-22T16:34:17.956752Z",
     "shell.execute_reply.started": "2026-01-22T16:34:17.953442Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:34:20.398128Z",
     "iopub.status.busy": "2026-01-22T16:34:20.397348Z",
     "iopub.status.idle": "2026-01-22T16:34:20.485491Z",
     "shell.execute_reply": "2026-01-22T16:34:20.484863Z",
     "shell.execute_reply.started": "2026-01-22T16:34:20.398097Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>Write a function to perform index wise multipl...</td>\n",
       "      <td>def index_multiplication(test_tup1, test_tup2)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>Write a python function to convert complex num...</td>\n",
       "      <td>import cmath  \\r\\ndef convert(numbers):    \\r\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>Write a function to create the next bigger num...</td>\n",
       "      <td>def rearrange_bigger(n):\\r\\n    nums = list(st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>Write a function to return the sum of all divi...</td>\n",
       "      <td>def sum_div(number):\\r\\n    divisors = [1]\\r\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>Write a python function to replace multiple oc...</td>\n",
       "      <td>import re \\r\\ndef replace(string, char): \\r\\n ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "444  Write a function to perform index wise multipl...   \n",
       "251  Write a python function to convert complex num...   \n",
       "406  Write a function to create the next bigger num...   \n",
       "294  Write a function to return the sum of all divi...   \n",
       "667  Write a python function to replace multiple oc...   \n",
       "\n",
       "                                                answer  \n",
       "444  def index_multiplication(test_tup1, test_tup2)...  \n",
       "251  import cmath  \\r\\ndef convert(numbers):    \\r\\...  \n",
       "406  def rearrange_bigger(n):\\r\\n    nums = list(st...  \n",
       "294  def sum_div(number):\\r\\n    divisors = [1]\\r\\n...  \n",
       "667  import re \\r\\ndef replace(string, char): \\r\\n ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "python_df = pd.read_json(path_or_buf= '/kaggle/input/mbppjsonl/mbpp.jsonl', lines=True)\n",
    "python_df = python_df[['text', 'code']]\n",
    "python_df.rename(columns = {'text': 'question', 'code': 'answer'}, inplace=True)\n",
    "python_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:34:22.182104Z",
     "iopub.status.busy": "2026-01-22T16:34:22.181123Z",
     "iopub.status.idle": "2026-01-22T16:34:22.208691Z",
     "shell.execute_reply": "2026-01-22T16:34:22.208196Z",
     "shell.execute_reply.started": "2026-01-22T16:34:22.182062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "with open('/kaggle/input/kaggle-data-1/kaggle_data.json','r') as f:\n",
    "    kaggle_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:34:22.725678Z",
     "iopub.status.busy": "2026-01-22T16:34:22.725271Z",
     "iopub.status.idle": "2026-01-22T16:34:22.768166Z",
     "shell.execute_reply": "2026-01-22T16:34:22.767593Z",
     "shell.execute_reply.started": "2026-01-22T16:34:22.725647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "kaggle_df = pd.DataFrame(kaggle_data)\n",
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r'<[^>]+>', '', text) # remove HTML/Markdown tags\n",
    "    text = re.sub(r'@\\w+', '', text) # remove @user tags\n",
    "    text = text.replace('\\n', ' ') # remove newline characters\n",
    "    text = re.sub(r'\\s+', ' ', text) # remove multiple spaces\n",
    "    text = text.strip() # remove leading and trailing spaces\n",
    "    return text\n",
    "\n",
    "kaggle_df['question'] = kaggle_df['question'].apply(clean_text)\n",
    "kaggle_df['answer'] = kaggle_df['answer'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:34:24.635375Z",
     "iopub.status.busy": "2026-01-22T16:34:24.634638Z",
     "iopub.status.idle": "2026-01-22T16:34:24.653758Z",
     "shell.execute_reply": "2026-01-22T16:34:24.653101Z",
     "shell.execute_reply.started": "2026-01-22T16:34:24.635339Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_science_df = pd.read_csv('/kaggle/input/data-science-1/DataScience QA.csv')\n",
    "data_science_df.rename(columns = {'Question': 'question', 'Answer': 'answer'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:34:25.260223Z",
     "iopub.status.busy": "2026-01-22T16:34:25.259592Z",
     "iopub.status.idle": "2026-01-22T16:34:25.264682Z",
     "shell.execute_reply": "2026-01-22T16:34:25.263977Z",
     "shell.execute_reply.started": "2026-01-22T16:34:25.260177Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "kaggle_df = pd.concat([kaggle_df, python_df, data_science_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:34:27.024803Z",
     "iopub.status.busy": "2026-01-22T16:34:27.024017Z",
     "iopub.status.idle": "2026-01-22T16:34:27.035549Z",
     "shell.execute_reply": "2026-01-22T16:34:27.034846Z",
     "shell.execute_reply.started": "2026-01-22T16:34:27.024767Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>Write a function to find the n‚Äôth carol number.</td>\n",
       "      <td>def get_carol(n): \\r\\n\\tresult = (2**n) - 1\\r\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>Getting HTML elements via XPath in bash</td>\n",
       "      <td>Getting HTML elements via XPath in bash from h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>Write a function to check whether the entered ...</td>\n",
       "      <td>def check_greater(arr, number):\\r\\n  arr.sort(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Write a function to find the kth element in th...</td>\n",
       "      <td>def kth_element(arr, n, k):\\r\\n  for i in rang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>Write a function to calculate the number of di...</td>\n",
       "      <td>def dig_let(s):\\r\\n d=l=0\\r\\n for c in s:\\r\\n ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "359    Write a function to find the n‚Äôth carol number.   \n",
       "398            Getting HTML elements via XPath in bash   \n",
       "432  Write a function to check whether the entered ...   \n",
       "100  Write a function to find the kth element in th...   \n",
       "342  Write a function to calculate the number of di...   \n",
       "\n",
       "                                                answer  \n",
       "359  def get_carol(n): \\r\\n\\tresult = (2**n) - 1\\r\\...  \n",
       "398  Getting HTML elements via XPath in bash from h...  \n",
       "432  def check_greater(arr, number):\\r\\n  arr.sort(...  \n",
       "100  def kth_element(arr, n, k):\\r\\n  for i in rang...  \n",
       "342  def dig_let(s):\\r\\n d=l=0\\r\\n for c in s:\\r\\n ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_limit(text: str) -> str:\n",
    "    max_len = 1024\n",
    "    return text[:max_len] if len(text) > max_len else text\n",
    "\n",
    "kaggle_df['question'] = kaggle_df['question'].apply(text_limit)\n",
    "kaggle_df['answer'] = kaggle_df['answer'].apply(text_limit)\n",
    "\n",
    "kaggle_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:34:31.158001Z",
     "iopub.status.busy": "2026-01-22T16:34:31.157669Z",
     "iopub.status.idle": "2026-01-22T16:34:31.243017Z",
     "shell.execute_reply": "2026-01-22T16:34:31.242362Z",
     "shell.execute_reply.started": "2026-01-22T16:34:31.157967Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\A'\n",
      "/tmp/ipykernel_55/501628493.py:4: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  kaggle_data.append(f'Question:\\n{row[\"question\"]}\\n\\Answer:\\n{row[\"answer\"]}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Question:\\nCreate a set from a series in pandas\\n\\\\Answer:\\nIf you only need to get list of unique values, you can just use unique method. If you want to have Python's set, then do set(some_series) In [1]: s = pd.Series([1, 2, 3, 1, 1, 4]) In [2]: s.unique() Out[2]: array([1, 2, 3, 4]) In [3]: set(s) Out[3]: {1, 2, 3, 4} However, if you have DataFrame, just select series out of it ( some_data_frame[''] ).\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_data = []\n",
    "\n",
    "for _, row in kaggle_df.iterrows():\n",
    "    kaggle_data.append(f'Question:\\n{row[\"question\"]}\\n\\Answer:\\n{row[\"answer\"]}')\n",
    "\n",
    "kaggle_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T20:55:15.700172Z",
     "iopub.status.busy": "2026-01-21T20:55:15.699433Z",
     "iopub.status.idle": "2026-01-21T20:55:16.243676Z",
     "shell.execute_reply": "2026-01-21T20:55:16.242893Z",
     "shell.execute_reply.started": "2026-01-21T20:55:15.700128Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenized_train_dataset = []\n",
    "for phrase in kaggle_data:\n",
    "  tokenized_train_dataset.append(tokenizer(phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T20:55:22.482321Z",
     "iopub.status.busy": "2026-01-21T20:55:22.481995Z",
     "iopub.status.idle": "2026-01-22T04:18:21.915442Z",
     "shell.execute_reply": "2026-01-22T04:18:21.914701Z",
     "shell.execute_reply.started": "2026-01-21T20:55:22.482292Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3912' max='3912' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3912/3912 7:22:46, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.020500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.820300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.724100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.810300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.731300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.580300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.709500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.607100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.679200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.724200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.584700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.463600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.409700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.486200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.427700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.331800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.490900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.416200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.392700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.225700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.197400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.136800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.252500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.193600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.198500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.218700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.172500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.188300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.952700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.930900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.974400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.959400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.897400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.951000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.075900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.865200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.714300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.762300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.753700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.682800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.641100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.792400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.717200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.657100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.726600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.764400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.605000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.581000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.601400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.524900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.532400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.533200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.527600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.532300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.493600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.491100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.382300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.412400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.412500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.421200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.382700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.455800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.404600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.349000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.336600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.334000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.303600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.310300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.314200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.336300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.339100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.310600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.321600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3912, training_loss=0.9184143191588193, metrics={'train_runtime': 26575.4377, 'train_samples_per_second': 0.588, 'train_steps_per_second': 0.147, 'total_flos': 3.672695632342426e+16, 'train_loss': 0.9184143191588193, 'epoch': 8.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "BETA_1 = 0.9\n",
    "BETA_2 = 0.999\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=\"./finetunedModelllama\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        num_train_epochs=8,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        adam_beta1=BETA_1,        \n",
    "        adam_beta2=BETA_2,\n",
    "        bf16=False,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_dir=\"./log\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_steps=50,\n",
    "        logging_steps=50,\n",
    "        report_to=\"none\"\n",
    "\n",
    "),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache=False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T04:18:41.569366Z",
     "iopub.status.busy": "2026-01-22T04:18:41.568111Z",
     "iopub.status.idle": "2026-01-22T04:19:15.722410Z",
     "shell.execute_reply": "2026-01-22T04:19:15.721647Z",
     "shell.execute_reply.started": "2026-01-22T04:18:41.569331Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Starting Export Process...\n",
      "üíæ Saving clean model to: ./final_clean_model...\n",
      "üöÄ Pushing to Hugging Face Hub: Rajesh17092004/Kaggle-Assistant-Pro-Llama-3.2-3B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90678c3b73d43509d0221a02cb43d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620a7d4b012c41ac8b39de069abbafc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d788384d664be6915805a3790b9306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a71093a34bd40b7b7eca1ff02fbc63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully pushed to Hugging Face!\n",
      "üì¶ Zipping for local download...\n",
      "\n",
      "üéâ ALL DONE!\n",
      "1. Model is live at: https://huggingface.co/Rajesh17092004/Kaggle-Assistant-Pro-Llama-3.2-3B\n",
      "2. Zip file is ready: /kaggle/working/finetuned_teacher_final.zip\n",
      "3. Zip Size: 345.97 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# We assume this is the Teacher model based on your previous messages\n",
    "repo_id = \"Rajesh17092004/Kaggle-Assistant-Pro-Llama-3.2-3B\"\n",
    "clean_output_dir = \"./final_clean_model\"\n",
    "\n",
    "print(\"‚è≥ Starting Export Process...\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 1: Save a \"Clean\" Version (Weights Only)\n",
    "# ---------------------------------------------------------\n",
    "# This strips away the massive optimizer states, reducing size by ~70%\n",
    "print(f\"üíæ Saving clean model to: {clean_output_dir}...\")\n",
    "\n",
    "# We use the 'trainer' object from your previous cell\n",
    "trainer.save_model(clean_output_dir)\n",
    "tokenizer.save_pretrained(clean_output_dir)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 2: Push to Hugging Face Hub\n",
    "# ---------------------------------------------------------\n",
    "print(f\"üöÄ Pushing to Hugging Face Hub: {repo_id}...\")\n",
    "\n",
    "try:\n",
    "    # Push the model and tokenizer we just saved\n",
    "    trainer.push_to_hub(repo_id)\n",
    "    tokenizer.push_to_hub(repo_id)\n",
    "    print(\"‚úÖ Successfully pushed to Hugging Face!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error pushing to Hub: {e}\")\n",
    "    print(\"Don't worry, we are still creating the Zip file below!\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 3: Zip the CLEAN folder for Local Download\n",
    "# ---------------------------------------------------------\n",
    "print(\"üì¶ Zipping for local download...\")\n",
    "\n",
    "zip_path = \"/kaggle/working/finetuned_teacher_final\"\n",
    "shutil.make_archive(zip_path, 'zip', root_dir=clean_output_dir)\n",
    "\n",
    "# Print final status\n",
    "zip_size_mb = os.path.getsize(zip_path + \".zip\") / (1024 * 1024)\n",
    "print(f\"\\nüéâ ALL DONE!\")\n",
    "print(f\"1. Model is live at: https://huggingface.co/{repo_id}\")\n",
    "print(f\"2. Zip file is ready: {zip_path}.zip\")\n",
    "print(f\"3. Zip Size: {zip_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T08:12:44.083629Z",
     "iopub.status.busy": "2026-01-22T08:12:44.083320Z",
     "iopub.status.idle": "2026-01-22T08:12:44.088251Z",
     "shell.execute_reply": "2026-01-22T08:12:44.087550Z",
     "shell.execute_reply.started": "2026-01-22T08:12:44.083591Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, LlamaTokenizer\n",
    "from huggingface_hub import notebook_login\n",
    "#from datasets import load_dataset\n",
    "from peft import PeftModel\n",
    "\n",
    "if \"COLAB_GPU\" in os.environ:\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T08:12:48.799830Z",
     "iopub.status.busy": "2026-01-22T08:12:48.799024Z",
     "iopub.status.idle": "2026-01-22T08:12:48.816515Z",
     "shell.execute_reply": "2026-01-22T08:12:48.815669Z",
     "shell.execute_reply.started": "2026-01-22T08:12:48.799794Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbac3dc04af94a96badab6dbea9e4a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if \"COLAB_GPU\" in os.environ:\n",
    "  !huggingface-cli login\n",
    "else:\n",
    "  notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:31:28.969656Z",
     "iopub.status.busy": "2026-01-22T16:31:28.969002Z",
     "iopub.status.idle": "2026-01-22T16:31:29.998995Z",
     "shell.execute_reply": "2026-01-22T16:31:29.998254Z",
     "shell.execute_reply.started": "2026-01-22T16:31:28.969626Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: bitsandbytes 0.49.1\n",
      "Uninstalling bitsandbytes-0.49.1:\n",
      "  Successfully uninstalled bitsandbytes-0.49.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:31:33.323296Z",
     "iopub.status.busy": "2026-01-22T16:31:33.322504Z",
     "iopub.status.idle": "2026-01-22T16:31:38.176594Z",
     "shell.execute_reply": "2026-01-22T16:31:38.175566Z",
     "shell.execute_reply.started": "2026-01-22T16:31:33.323256Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (26.0rc2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "Using cached bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.49.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:31:54.456694Z",
     "iopub.status.busy": "2026-01-22T16:31:54.456329Z",
     "iopub.status.idle": "2026-01-22T16:32:49.570610Z",
     "shell.execute_reply": "2026-01-22T16:32:49.570043Z",
     "shell.execute_reply.started": "2026-01-22T16:31:54.456658Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40f1e4540204949881f5ff1a8e47ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca72dfa5821348aa928ba5ead3f7a535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7dac42409ae4db98189cdac61fce5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23d7e8123e74d49b2abc8b5b40f47e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a70533a0664410af5adb6571741e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a423e55853cb4bb889088d992c3ed991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc1364ec5004a5fb7b4e6f87b3a4340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config,device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:33:29.888572Z",
     "iopub.status.busy": "2026-01-22T16:33:29.887826Z",
     "iopub.status.idle": "2026-01-22T16:33:31.113006Z",
     "shell.execute_reply": "2026-01-22T16:33:31.112339Z",
     "shell.execute_reply.started": "2026-01-22T16:33:29.888541Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57c334279c54ca9a620ee26e98a08b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b6babb12e74f0bb1a5f7ab04ef0c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c40186c36d748b9ba400781fdfa22d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True) # 2. Use AutoTokenizer\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:37:12.873064Z",
     "iopub.status.busy": "2026-01-22T16:37:12.872675Z",
     "iopub.status.idle": "2026-01-22T16:37:13.427170Z",
     "shell.execute_reply": "2026-01-22T16:37:13.426363Z",
     "shell.execute_reply.started": "2026-01-22T16:37:12.873019Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenized_train_dataset = []\n",
    "for phrase in kaggle_data:\n",
    "  tokenized_train_dataset.append(tokenizer(phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:37:16.673215Z",
     "iopub.status.busy": "2026-01-22T16:37:16.672370Z",
     "iopub.status.idle": "2026-01-22T16:37:16.678292Z",
     "shell.execute_reply": "2026-01-22T16:37:16.677382Z",
     "shell.execute_reply.started": "2026-01-22T16:37:16.673157Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_dir_size(path='.'):\n",
    "    \"\"\"Calculates the size of a directory in megabytes.\"\"\"\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size / (1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:37:22.893683Z",
     "iopub.status.busy": "2026-01-22T16:37:22.893090Z",
     "iopub.status.idle": "2026-01-22T16:37:44.757987Z",
     "shell.execute_reply": "2026-01-22T16:37:44.757088Z",
     "shell.execute_reply.started": "2026-01-22T16:37:22.893653Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading adapter from: /kaggle/input/teacher-weights/...\n",
      "Merging adapter into the base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge complete.\n",
      "Saving merged model to: /kaggle/working/merged_finetuned_model...\n",
      "Save complete.\n",
      "Calculating size of the merged model on disk...\n",
      "------------------------------\n",
      "Full Merged Model Size: 2138.87 MB\n",
      "------------------------------\n",
      "Cleaning up by deleting: /kaggle/working/merged_finetuned_model\n",
      "Cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "from peft import PeftModel\n",
    "\n",
    "adapter_path = \"/kaggle/input/teacher-weights/\"\n",
    "merged_model_save_path = \"/kaggle/working/merged_finetuned_model\"\n",
    "\n",
    "print(f\"Loading adapter from: {adapter_path}...\")\n",
    "\n",
    "fine_tuned_model = PeftModel.from_pretrained(model, adapter_path)\n",
    "\n",
    "print(\"Merging adapter into the base model...\")\n",
    "merged_model = fine_tuned_model.merge_and_unload()\n",
    "print(\"Merge complete.\")\n",
    "\n",
    "print(f\"Saving merged model to: {merged_model_save_path}...\")\n",
    "merged_model.save_pretrained(merged_model_save_path)\n",
    "print(\"Save complete.\")\n",
    "\n",
    "print(\"Calculating size of the merged model on disk...\")\n",
    "\n",
    "try:\n",
    "    disk_size_mb = get_dir_size(merged_model_save_path)\n",
    "    print(\"---\" * 10)\n",
    "    print(f\"Full Merged Model Size: {disk_size_mb:.2f} MB\")\n",
    "    print(\"---\" * 10)\n",
    "except NameError:\n",
    "    print(\"\\n[Error] The 'get_dir_size' function is not defined.\")\n",
    "    print(\"Please run the cell that defines 'get_dir_size' and try again.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")\n",
    "\n",
    "print(f\"Cleaning up by deleting: {merged_model_save_path}\")\n",
    "if os.path.exists(merged_model_save_path):\n",
    "    shutil.rmtree(merged_model_save_path)\n",
    "    print(\"Cleanup complete.\")\n",
    "else:\n",
    "    print(\"Cleanup not needed, directory does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:38:03.747229Z",
     "iopub.status.busy": "2026-01-22T16:38:03.746567Z",
     "iopub.status.idle": "2026-01-22T16:38:03.753886Z",
     "shell.execute_reply": "2026-01-22T16:38:03.753239Z",
     "shell.execute_reply.started": "2026-01-22T16:38:03.747176Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def measure_latency(model, tokenizer, query=\"Explain the concept of overfitting. Why is it a problem, and what is one common technique to prevent it?\"):\n",
    "    \"\"\"Measures the average latency of the model over several runs.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": query}]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    latencies = []\n",
    "    print(\"Running warmup...\")\n",
    "    for _ in range(5):\n",
    "        _ = model.generate(**inputs, max_new_tokens=50, eos_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    print(\"Running timed benchmark...\")\n",
    "    for _ in range(20):\n",
    "        start_time = time.perf_counter()\n",
    "        _ = model.generate(**inputs, max_new_tokens=50, eos_token_id=tokenizer.eos_token_id)\n",
    "        latency = (time.perf_counter() - start_time) * 1000 # Convert to milliseconds\n",
    "        latencies.append(latency)\n",
    "        \n",
    "    avg_latency = np.mean(latencies)\n",
    "    std_latency = np.std(latencies)\n",
    "    return {\"avg_latency_ms\": avg_latency, \"std_latency_ms\": std_latency}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:38:09.270316Z",
     "iopub.status.busy": "2026-01-22T16:38:09.269423Z",
     "iopub.status.idle": "2026-01-22T16:38:09.274120Z",
     "shell.execute_reply": "2026-01-22T16:38:09.273418Z",
     "shell.execute_reply.started": "2026-01-22T16:38:09.270275Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:38:11.552271Z",
     "iopub.status.busy": "2026-01-22T16:38:11.551419Z",
     "iopub.status.idle": "2026-01-22T16:39:58.217423Z",
     "shell.execute_reply": "2026-01-22T16:39:58.216550Z",
     "shell.execute_reply.started": "2026-01-22T16:38:11.552226Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEACHER MODEL LATENCY\n",
      "Running warmup...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running timed benchmark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MERGED Teacher Latency: 4232.06 +/- 40.10 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"TEACHER MODEL LATENCY\")\n",
    "\n",
    "merged_teacher_latency = measure_latency(merged_model, tokenizer)\n",
    "\n",
    "print(f\"MERGED Teacher Latency: {merged_teacher_latency['avg_latency_ms']:.2f} +/- {merged_teacher_latency['std_latency_ms']:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:40:02.475047Z",
     "iopub.status.busy": "2026-01-22T16:40:02.474302Z",
     "iopub.status.idle": "2026-01-22T16:40:04.466529Z",
     "shell.execute_reply": "2026-01-22T16:40:04.465640Z",
     "shell.execute_reply.started": "2026-01-22T16:40:02.474999Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "/tmp/ipykernel_55/2090662432.py:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "  eval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "What is kaggle?\n",
      "\\Just answer this question accurately and concisely.\n",
      "Kaggle is a platform for data science competitions, hosting datasets to be used in such competitions.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"What is kaggle?\"\n",
    "\n",
    "eval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n",
    "\n",
    "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "fine_tuned_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  print(tokenizer.decode(fine_tuned_model.generate(**promptTokenized, max_new_tokens=256,repetition_penalty=1.2,eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True))\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:40:07.511210Z",
     "iopub.status.busy": "2026-01-22T16:40:07.510879Z",
     "iopub.status.idle": "2026-01-22T16:40:08.983544Z",
     "shell.execute_reply": "2026-01-22T16:40:08.982703Z",
     "shell.execute_reply.started": "2026-01-22T16:40:07.511159Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "/tmp/ipykernel_55/3007961873.py:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "  eval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Write a Python function called find_max that takes a list of numbers as input and returns the largest number in the list.\n",
      "\\Just answer this question accurately and concisely.\n",
      "Answer: \n",
      "def find_max(lst):\n",
      "    return max(lst)\n"
     ]
    }
   ],
   "source": [
    "user_question = \"Write a Python function called find_max that takes a list of numbers as input and returns the largest number in the list.\"\n",
    "\n",
    "eval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n",
    "\n",
    "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "fine_tuned_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  print(tokenizer.decode(fine_tuned_model.generate(**promptTokenized, max_new_tokens=256,repetition_penalty=1.2,eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True))\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:40:11.495853Z",
     "iopub.status.busy": "2026-01-22T16:40:11.495496Z",
     "iopub.status.idle": "2026-01-22T16:40:13.209609Z",
     "shell.execute_reply": "2026-01-22T16:40:13.208718Z",
     "shell.execute_reply.started": "2026-01-22T16:40:11.495822Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "/tmp/ipykernel_55/3710983772.py:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "  eval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Write a Python function called is_even that takes an integer as input and returns True if the number is even and False otherwise\n",
      "\\Just answer this question accurately and concisely.\n",
      "Answer: \n",
      "def is_even(n):\n",
      "    return n % 2 == 0\n"
     ]
    }
   ],
   "source": [
    "user_question = \"Write a Python function called is_even that takes an integer as input and returns True if the number is even and False otherwise\"\n",
    "\n",
    "eval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n",
    "\n",
    "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "fine_tuned_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  print(tokenizer.decode(fine_tuned_model.generate(**promptTokenized, max_new_tokens=256,repetition_penalty=1.2,eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True))\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:40:15.835161Z",
     "iopub.status.busy": "2026-01-22T16:40:15.834321Z",
     "iopub.status.idle": "2026-01-22T16:40:22.153380Z",
     "shell.execute_reply": "2026-01-22T16:40:22.152675Z",
     "shell.execute_reply.started": "2026-01-22T16:40:15.835118Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "/tmp/ipykernel_55/4058350022.py:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "  eval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "What is the difference between classification and regression in machine learning? Can you give an example of each?\n",
      "\\Just answer this question accurately and concisely.\n",
      "Classification: Assigns a categorical label to input data. Regression: Predicts continuous values for input variables.\n",
      "\n",
      "Example Classification: Breast Cancer Diagnosis - A model predicts whether breast cancer is malignant or benign based on certain features (e.g., tumor size, cell count). Example Regression: House Prices Prediction - A model predicts house prices based on factors like location, number of bedrooms, etc.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"What is the difference between classification and regression in machine learning? Can you give an example of each?\"\n",
    "\n",
    "eval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n",
    "\n",
    "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "fine_tuned_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  print(tokenizer.decode(fine_tuned_model.generate(**promptTokenized, max_new_tokens=256,repetition_penalty=1.2,eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True))\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:40:24.785168Z",
     "iopub.status.busy": "2026-01-22T16:40:24.784532Z",
     "iopub.status.idle": "2026-01-22T16:40:32.846400Z",
     "shell.execute_reply": "2026-01-22T16:40:32.845748Z",
     "shell.execute_reply.started": "2026-01-22T16:40:24.785139Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "/tmp/ipykernel_55/890702873.py:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "  eval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Explain the concept of overfitting. Why is it a problem, and what is one common technique to prevent it?\n",
      "\\Just answer this question accurately and concisely.\n",
      "Overfitting occurs when a model becomes too specialized in fitting the noise present in training data rather than generalizing well to unseen or test data. This can happen if we have more parameters than necessary for our dataset size, leading the model to memorize specific patterns instead of learning meaningful relationships between variables.\n",
      "\n",
      "One common technique to prevent overfitting is regularization, which adds a penalty term to the loss function that discourages large weights by increasing the cost of complex models. For example, L1 (L\n"
     ]
    }
   ],
   "source": [
    "user_question = \"Explain the concept of overfitting. Why is it a problem, and what is one common technique to prevent it?\"\n",
    "\n",
    "eval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n",
    "\n",
    "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "fine_tuned_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  print(tokenizer.decode(fine_tuned_model.generate(**promptTokenized, max_new_tokens=100,repetition_penalty=1.2,eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True))\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:40:35.774872Z",
     "iopub.status.busy": "2026-01-22T16:40:35.774572Z",
     "iopub.status.idle": "2026-01-22T16:40:40.710750Z",
     "shell.execute_reply": "2026-01-22T16:40:40.710068Z",
     "shell.execute_reply.started": "2026-01-22T16:40:35.774844Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "/tmp/ipykernel_55/270570122.py:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "  eval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "What is a p-value, and what does it typically signify in the context of hypothesis testing?\n",
      "\\Just answer this question accurately and concisely.\n",
      "Answer:  A p-value represents the probability of observing an effect at least as extreme as that observed during an experiment or study, assuming no real effect exists. It signifies whether there's sufficient evidence to reject a null hypothesis based on statistical significance levels (usually Œ± = 0.05).\n"
     ]
    }
   ],
   "source": [
    "user_question = \"What is a p-value, and what does it typically signify in the context of hypothesis testing?\"\n",
    "\n",
    "eval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n",
    "\n",
    "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "fine_tuned_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  print(tokenizer.decode(fine_tuned_model.generate(**promptTokenized, max_new_tokens=256,repetition_penalty=1.2,eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True))\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:40:42.826404Z",
     "iopub.status.busy": "2026-01-22T16:40:42.825833Z",
     "iopub.status.idle": "2026-01-22T16:40:53.531411Z",
     "shell.execute_reply": "2026-01-22T16:40:53.530741Z",
     "shell.execute_reply.started": "2026-01-22T16:40:42.826373Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "/tmp/ipykernel_55/1962209463.py:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "  eval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "how can i become a grandmaster in kaggle?\n",
      "\\Just answer this question accurately and concisely.\n",
      "To become a Grand Master on Kaggle, you need to achieve the top rank (100%) across all competitions. Here are some steps that could help:\n",
      "\n",
      "*   **Participate consistently**: Regularly participate in competitions to gain experience and improve your skills.\n",
      "*   **Focus on accuracy**: Aim for high accuracy scores by carefully selecting models, tuning hyperparameters, and testing different approaches.\n",
      "*   **Stay up-to-date with trends**: Keep track of emerging trends and techniques in machine learning to stay ahead of the competition.\n",
      "\n",
      "Becoming a Grand Master takes time, dedication, and persistence. Focus on continuous improvement and strive to excel in each competition. Good luck!\n"
     ]
    }
   ],
   "source": [
    "user_question = \"how can i become a grandmaster in kaggle?\"\n",
    "\n",
    "eval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n",
    "\n",
    "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "fine_tuned_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  print(tokenizer.decode(fine_tuned_model.generate(**promptTokenized, max_new_tokens=256,repetition_penalty=1.2,eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True))\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DISTILLATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:40:53.533127Z",
     "iopub.status.busy": "2026-01-22T16:40:53.532735Z",
     "iopub.status.idle": "2026-01-22T16:41:03.740511Z",
     "shell.execute_reply": "2026-01-22T16:41:03.739967Z",
     "shell.execute_reply.started": "2026-01-22T16:40:53.533074Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e0bb2304b94cda920752a131ac91c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "techer_model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config,device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:41:21.073595Z",
     "iopub.status.busy": "2026-01-22T16:41:21.073187Z",
     "iopub.status.idle": "2026-01-22T16:41:21.643373Z",
     "shell.execute_reply": "2026-01-22T16:41:21.642620Z",
     "shell.execute_reply.started": "2026-01-22T16:41:21.073566Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True) \n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:41:25.269524Z",
     "iopub.status.busy": "2026-01-22T16:41:25.269175Z",
     "iopub.status.idle": "2026-01-22T16:41:27.779124Z",
     "shell.execute_reply": "2026-01-22T16:41:27.778315Z",
     "shell.execute_reply.started": "2026-01-22T16:41:25.269498Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "teacher_model = PeftModel.from_pretrained(techer_model,\"/kaggle/input/teacher-weights/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:41:56.027474Z",
     "iopub.status.busy": "2026-01-22T16:41:56.027012Z",
     "iopub.status.idle": "2026-01-22T16:41:56.046236Z",
     "shell.execute_reply": "2026-01-22T16:41:56.045418Z",
     "shell.execute_reply.started": "2026-01-22T16:41:56.027423Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher model loaded and frozen.\n"
     ]
    }
   ],
   "source": [
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "teacher_model.eval()\n",
    "print(\"Teacher model loaded and frozen.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:41:58.913926Z",
     "iopub.status.busy": "2026-01-22T16:41:58.913570Z",
     "iopub.status.idle": "2026-01-22T16:42:09.970044Z",
     "shell.execute_reply": "2026-01-22T16:42:09.969238Z",
     "shell.execute_reply.started": "2026-01-22T16:41:58.913898Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e4e900648d4280aca733ad8cb8d6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b85139bd27343caab3ba357a8f442b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fcd1edbc00945548003d31424498ec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "student_model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "student_model = AutoModelForCausalLM.from_pretrained(student_model_id, quantization_config=bnb_config,device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:42:20.277099Z",
     "iopub.status.busy": "2026-01-22T16:42:20.276163Z",
     "iopub.status.idle": "2026-01-22T16:42:20.281010Z",
     "shell.execute_reply": "2026-01-22T16:42:20.280298Z",
     "shell.execute_reply.started": "2026-01-22T16:42:20.277057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import get_peft_model,LoraConfig,prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:42:23.343952Z",
     "iopub.status.busy": "2026-01-22T16:42:23.343478Z",
     "iopub.status.idle": "2026-01-22T16:42:24.549556Z",
     "shell.execute_reply": "2026-01-22T16:42:24.548663Z",
     "shell.execute_reply.started": "2026-01-22T16:42:23.343904Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "student_model.gradient_checkpointing_enable()\n",
    "student_model = prepare_model_for_kbit_training(student_model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "student_model = get_peft_model(student_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:42:26.632127Z",
     "iopub.status.busy": "2026-01-22T16:42:26.631828Z",
     "iopub.status.idle": "2026-01-22T16:42:26.636715Z",
     "shell.execute_reply": "2026-01-22T16:42:26.635881Z",
     "shell.execute_reply.started": "2026-01-22T16:42:26.632100Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T16:42:36.433815Z",
     "iopub.status.busy": "2026-01-22T16:42:36.433473Z",
     "iopub.status.idle": "2026-01-22T17:59:32.085740Z",
     "shell.execute_reply": "2026-01-22T17:59:32.084991Z",
     "shell.execute_reply.started": "2026-01-22T16:42:36.433773Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/1249504526.py:7: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `DistillationTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DistillationTrainer is configured and ready for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='369' max='369' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [369/369 1:16:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>7184.609400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5316.736900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>4827.927500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4303.109700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4204.884400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4094.101900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>4217.531600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Distillation training has completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        outputs_stu = model(**inputs)\n",
    "        logits_stu = outputs_stu.logits\n",
    "        with torch.no_grad():\n",
    "            outputs_tea = self.teacher_model(**inputs)\n",
    "            logits_tea = outputs_tea.logits\n",
    "        loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        loss_kd = self.args.temperature ** 2 * loss_fct(\n",
    "            F.log_softmax(logits_stu / self.args.temperature, dim=-1),\n",
    "            F.softmax(logits_tea / self.args.temperature, dim=-1)\n",
    "        )\n",
    "\n",
    "        return (loss_kd, outputs_stu) if return_outputs else loss_kd\n",
    "\n",
    "class DistillationTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.temperature = temperature\n",
    "\n",
    "training_args = DistillationTrainingArguments(\n",
    "    output_dir=\"./distilled_model_checkpoints\",\n",
    "    num_train_epochs=3,                 \n",
    "    per_device_train_batch_size=1,      \n",
    "    gradient_accumulation_steps=16,\n",
    "    fp16=True,\n",
    "    learning_rate=5e-5,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    temperature=2.0,                    \n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "distillation_trainer = DistillationTrainer(\n",
    "    model=student_model,              \n",
    "    teacher_model=teacher_model,       \n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,    \n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\" DistillationTrainer is configured and ready for training.\")\n",
    "distillation_trainer.train()\n",
    "print(\"‚úÖDistillation training has completed successfully!\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T18:01:14.641660Z",
     "iopub.status.busy": "2026-01-22T18:01:14.640657Z",
     "iopub.status.idle": "2026-01-22T18:01:43.385832Z",
     "shell.execute_reply": "2026-01-22T18:01:43.384893Z",
     "shell.execute_reply.started": "2026-01-22T18:01:14.641626Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found candidate dirs (last few):\n",
      "  distilled_model_checkpoints/checkpoint-123\n",
      "  distilled_model_checkpoints/checkpoint-246\n",
      "  distilled_model_checkpoints/checkpoint-369\n",
      "Using checkpoint folder: distilled_model_checkpoints/checkpoint-369\n",
      "Zipped to: /kaggle/working/distilled_model_final.zip\n",
      "Size (bytes): 498554212\n"
     ]
    }
   ],
   "source": [
    "import os, glob, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR = Path(\"./distilled_model_checkpoints\")\n",
    "\n",
    "ckpts = sorted([p for p in OUT_DIR.glob(\"**/*\") if p.is_dir() and (\"checkpoint\" in p.name or p.name==\"finetunedModel\" or p.name==\"adapter\")])\n",
    "print(\"Found candidate dirs (last few):\")\n",
    "for x in ckpts[-10:]:\n",
    "    print(\" \", x)\n",
    "\n",
    "checkpoint_dirs = sorted([d for d in OUT_DIR.iterdir() if d.is_dir() and (\"checkpoint\" in d.name or d.name==\"adapter\")])\n",
    "if len(checkpoint_dirs) > 0:\n",
    "    latest_ckpt = checkpoint_dirs[-1]\n",
    "else:\n",
    "    latest_ckpt = OUT_DIR \n",
    "\n",
    "print(\"Using checkpoint folder:\", latest_ckpt)\n",
    "zip_name = \"/kaggle/working/distilled_model_final\"\n",
    "shutil.make_archive(zip_name, 'zip', root_dir=str(latest_ckpt))\n",
    "print(\"Zipped to:\", zip_name + \".zip\")\n",
    "print(\"Size (bytes):\", os.path.getsize(zip_name + \".zip\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T18:02:20.000640Z",
     "iopub.status.busy": "2026-01-22T18:02:20.000001Z",
     "iopub.status.idle": "2026-01-22T18:02:23.466423Z",
     "shell.execute_reply": "2026-01-22T18:02:23.465794Z",
     "shell.execute_reply.started": "2026-01-22T18:02:20.000609Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "distilled_model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "distilled_model = AutoModelForCausalLM.from_pretrained(distilled_model_id, quantization_config=bnb_config,device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T18:02:42.191919Z",
     "iopub.status.busy": "2026-01-22T18:02:42.191306Z",
     "iopub.status.idle": "2026-01-22T18:02:44.221243Z",
     "shell.execute_reply": "2026-01-22T18:02:44.220320Z",
     "shell.execute_reply.started": "2026-01-22T18:02:42.191890Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3975382db6864737b63d68f86ea49b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b4f2be18a04de4a6fdcfb9e9d589d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb89f4d7a3b47fdb143fb042ce6c5f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(distilled_model_id, trust_remote_code=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "from peft import PeftModel\n",
    "distilled_model = PeftModel.from_pretrained(distilled_model,\"distilled_model_checkpoints/checkpoint-369\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T18:03:32.787462Z",
     "iopub.status.busy": "2026-01-22T18:03:32.787112Z",
     "iopub.status.idle": "2026-01-22T18:03:38.627913Z",
     "shell.execute_reply": "2026-01-22T18:03:38.627054Z",
     "shell.execute_reply.started": "2026-01-22T18:03:32.787433Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "/tmp/ipykernel_55/1373154909.py:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "  eval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "how can i become a grandmaster in kaggle?\n",
      "\\Just answer this question accurately and concisely.\n",
      "\\Answer:\n",
      "To be a Grand Master on Kaggle, you need to have at least 1000 points. You can achieve that by participating in the competition with your best performance or by getting high scores in specific competitions. If you want more details, I'd recommend checking out the official Kaggle website for more information. (Source: https://www.kaggle.com/ )\n"
     ]
    }
   ],
   "source": [
    "user_question = \"how can i become a grandmaster in kaggle?\"\n",
    "\n",
    "eval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n",
    "\n",
    "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "distilled_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  print(tokenizer.decode(distilled_model.generate(**promptTokenized, max_new_tokens=256,repetition_penalty=1.2,eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True))\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T18:05:06.152488Z",
     "iopub.status.busy": "2026-01-22T18:05:06.152117Z",
     "iopub.status.idle": "2026-01-22T18:05:24.523130Z",
     "shell.execute_reply": "2026-01-22T18:05:24.522444Z",
     "shell.execute_reply.started": "2026-01-22T18:05:06.152457Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "/tmp/ipykernel_55/2678165030.py:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "  eval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Write a Python function called find_max that takes a list of numbers as input and returns the largest number in the list.\n",
      "\\Just answer this question accurately and concisely.\n",
      "\\Answer:\n",
      "def find_max(list1): \n",
      "    max = max(list1) \n",
      "    return max  # Return the maximum value from the list. ‚Äã\n",
      "    # If you want to get all values, use list(max(list1)) ‚Äã\n",
      "    # or use list(map(max, list1)) ‚Äã\n",
      "    # or use list(sorted(list1)) ‚Äã\n",
      "    # Or just use max(list1) if you have only one element in your list. ‚Äã\n",
      "    # For example: print(find_max([3,2,4,5,6]))  # Output: 6 ``` This is a simple solution using the built-in max() function which returns the highest value in the given list. You can also write it yourself by using the following code: def find_max(list1): \n",
      "    max = list1[0] \n",
      "    for i in range(1, len(list1)): \n",
      "        if list1[i] > max: \n",
      "            max = list1[i] \n",
      "    return max  # Return the maximum value from the list. ‚Äã\n",
      "\n",
      "# Test the function with an example: print(find_max([3,2,4,5,6]) )  # Output: 6 ‚Äã\n",
      "# Alternatively, you can use map\n"
     ]
    }
   ],
   "source": [
    "user_question = \"Write a Python function called find_max that takes a list of numbers as input and returns the largest number in the list.\"\n",
    "\n",
    "eval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n",
    "\n",
    "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "distilled_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  print(tokenizer.decode(distilled_model.generate(**promptTokenized, max_new_tokens=256,repetition_penalty=1.2,eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True))\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T11:03:32.532779Z",
     "iopub.status.busy": "2025-10-15T11:03:32.532456Z",
     "iopub.status.idle": "2025-10-15T11:03:50.174139Z",
     "shell.execute_reply": "2025-10-15T11:03:50.173185Z",
     "shell.execute_reply.started": "2025-10-15T11:03:32.532759Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Write a Python function called is_even that takes an integer as input and returns True if the number is even and False otherwise.\n",
      "\\Just answer this question accurately and concisely.\n",
      "\\Answer:\n",
      "def is_even(n): \n",
      "    return n % 2 == 0;  # Returns true if n is even, false else.  # Using lambda function for simplicity.  def is_even(n): return (lambda x: x % 2 == 0) (n)  # The above code can be written in one line using lambda function.  # Example use case: print(is_even(10))  # Output: True  # Example use case: print(is_even(-1))  # Output: False  # Example use case: print(is_even(3))  # Output: True  # Example use case: print(is_even(4))  # Output: True  # Example use case: print(is_even(5))  # Output: False  # Example use case: print(is_even(6))  # Output: True  # Example use case: print(is_even(7))  # Output: False  # Example use case: print(is_even(8))  # Output: True  # Example use case: print(is_even(9))  # Output: False  # Example use case: print(is_even(10))  # Output: True  # Example use case: print(is_even(\n"
     ]
    }
   ],
   "source": [
    "user_question = \"Write a Python function called is_even that takes an integer as input and returns True if the number is even and False otherwise.\"\n",
    "\n",
    "eval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n",
    "\n",
    "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "distilled_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  print(tokenizer.decode(distilled_model.generate(**promptTokenized, max_new_tokens=256,repetition_penalty=1.2,eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True))\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T18:08:22.774700Z",
     "iopub.status.busy": "2026-01-22T18:08:22.774034Z",
     "iopub.status.idle": "2026-01-22T18:08:29.822436Z",
     "shell.execute_reply": "2026-01-22T18:08:29.821629Z",
     "shell.execute_reply.started": "2026-01-22T18:08:22.774667Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "/tmp/ipykernel_55/1021915395.py:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "  eval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "what is the difference between the classification and regression in machine learning?\n",
      "\\Just answer this question accurately and concisely.\n",
      "\\Answer:\n",
      "Classification problems are those where you have a set of input data and you want to predict which class or category that data belongs to. Regression problems, on the other hand, are those where you want to predict a continuous value (e.g., price of an item). In general, if you're trying to classify something into one of several categories, you'll use a classifier. If you're trying to forecast a continuous value, you'll use a regressor.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"what is the difference between the classification and regression in machine learning?\"\n",
    "\n",
    "eval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n",
    "\n",
    "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "distilled_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  print(tokenizer.decode(distilled_model.generate(**promptTokenized, max_new_tokens=256,repetition_penalty=1.2,eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True))\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T18:09:09.225475Z",
     "iopub.status.busy": "2026-01-22T18:09:09.224784Z",
     "iopub.status.idle": "2026-01-22T18:09:18.493410Z",
     "shell.execute_reply": "2026-01-22T18:09:18.492692Z",
     "shell.execute_reply.started": "2026-01-22T18:09:09.225444Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "/tmp/ipykernel_55/181090311.py:3: SyntaxWarning: invalid escape sequence '\\J'\n",
      "  eval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Explain the concept of overfitting. Why is it a problem, and what is one common technique to prevent it?\n",
      "\\Just answer this question accurately and concisely.\n",
      "\\Answer:\n",
      "Overfitting occurs when a model becomes too specialized in the training data and fails to generalize well to new, unseen data. This can happen if the model has too many parameters or if there are too many features in the dataset. One common technique to prevent overfitting is to use regularization techniques such as L1 or L2 regularization, which add penalty terms to the loss function to discourage large parameter values. Another approach is to use ensemble methods, which combine the predictions of multiple models to improve generalizability. Finally, using more data or increasing the size of the training set can help reduce overfitting.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"Explain the concept of overfitting. Why is it a problem, and what is one common technique to prevent it?\"\n",
    "\n",
    "eval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n",
    "\n",
    "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "distilled_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  print(tokenizer.decode(distilled_model.generate(**promptTokenized, max_new_tokens=256,repetition_penalty=1.2,eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True))\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T18:13:54.196890Z",
     "iopub.status.busy": "2026-01-22T18:13:54.196554Z",
     "iopub.status.idle": "2026-01-22T18:14:04.671006Z",
     "shell.execute_reply": "2026-01-22T18:14:04.670252Z",
     "shell.execute_reply.started": "2026-01-22T18:13:54.196859Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base student model: meta-llama/Llama-3.2-1B-Instruct...\n",
      "Loading student adapter from: distilled_model_checkpoints/checkpoint-369...\n",
      "Merging student adapter into the base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge complete.\n",
      "Saving merged student model to: /kaggle/working/merged_distilled_model...\n",
      "Save complete.\n",
      "Calculating size of the merged student model on disk...\n",
      "------------------------------\n",
      "Full Merged DISTILLED STUDENT Model Size: 980.07 MB\n",
      "------------------------------\n",
      "Cleaning up by deleting: /kaggle/working/merged_distilled_model\n",
      "Cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "student_base_model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "student_adapter_path = \"distilled_model_checkpoints/checkpoint-369\"\n",
    "merged_student_save_path = \"/kaggle/working/merged_distilled_model\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print(f\"Loading base student model: {student_base_model_id}...\")\n",
    "base_student_model = AutoModelForCausalLM.from_pretrained(\n",
    "    student_base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"Loading student adapter from: {student_adapter_path}...\")\n",
    "distilled_peft_model = PeftModel.from_pretrained(base_student_model, student_adapter_path)\n",
    "\n",
    "print(\"Merging student adapter into the base model...\")\n",
    "merged_student_model = distilled_peft_model.merge_and_unload()\n",
    "print(\"Merge complete.\")\n",
    "print(f\"Saving merged student model to: {merged_student_save_path}...\")\n",
    "merged_student_model.save_pretrained(merged_student_save_path)\n",
    "print(\"Save complete.\")\n",
    "print(\"Calculating size of the merged student model on disk...\")\n",
    "try:\n",
    "    student_disk_size_mb = get_dir_size(merged_student_save_path)\n",
    "    print(\"---\" * 10)\n",
    "    print(f\"Full Merged DISTILLED STUDENT Model Size: {student_disk_size_mb:.2f} MB\")\n",
    "    print(\"---\" * 10)\n",
    "except NameError:\n",
    "    print(\"\\n[Error] The 'get_dir_size' function is not defined.\")\n",
    "    print(\"Please run the cell that defines 'get_dir_size' and try again.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")\n",
    "print(f\"Cleaning up by deleting: {merged_student_save_path}\")\n",
    "if os.path.exists(merged_student_save_path):\n",
    "    shutil.rmtree(merged_student_save_path)\n",
    "    print(\"Cleanup complete.\")\n",
    "else:\n",
    "    print(\"Cleanup not needed, directory does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T18:14:33.397944Z",
     "iopub.status.busy": "2026-01-22T18:14:33.397143Z",
     "iopub.status.idle": "2026-01-22T18:15:30.752516Z",
     "shell.execute_reply": "2026-01-22T18:15:30.751813Z",
     "shell.execute_reply.started": "2026-01-22T18:14:33.397911Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STUDENT MODEL LATENCY\n",
      "Running warmup...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running timed benchmark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MERGED Student Latency: 2290.54 +/- 24.96 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"STUDENT MODEL LATENCY\")\n",
    "merged_student_latency = measure_latency(merged_student_model, tokenizer)\n",
    "print(f\"MERGED Student Latency: {merged_student_latency['avg_latency_ms']:.2f} +/- {merged_student_latency['std_latency_ms']:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MERGED Teacher Latency: 4232.06 +/- 40.10 ms\n",
    "MERGED Student Latency: 2290.54 +/- 24.96 ms\n",
    "\n",
    "Full Merged Model Size: 2138.87 MB \n",
    "Full Merged DISTILLED STUDENT Model Size: 980.07 MB"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4102922,
     "sourceId": 7114700,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9306473,
     "sourceId": 14569523,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9306486,
     "sourceId": 14569541,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9306519,
     "sourceId": 14569585,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9312524,
     "sourceId": 14578653,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
