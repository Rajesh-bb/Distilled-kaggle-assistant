{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13375179,"sourceType":"datasetVersion","datasetId":8485762},{"sourceId":13375183,"sourceType":"datasetVersion","datasetId":8485764},{"sourceId":13375186,"sourceType":"datasetVersion","datasetId":8485766}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nimport os\n\n# The path to the directory you want to delete\ndirectory_path = \"/kaggle/working/finetunedModel\"\n\n# Safety check: only try to delete it if it actually exists\nif os.path.exists(directory_path):\n    # This command deletes the entire folder and all its contents\n    shutil.rmtree(directory_path)\n    print(f\"Successfully deleted directory: {directory_path}\")\nelse:\n    print(f\"Directory not found, nothing to delete: {directory_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T11:56:06.066077Z","iopub.execute_input":"2025-10-24T11:56:06.066408Z","iopub.status.idle":"2025-10-24T11:56:06.833891Z","shell.execute_reply.started":"2025-10-24T11:56:06.066383Z","shell.execute_reply":"2025-10-24T11:56:06.833054Z"}},"outputs":[{"name":"stdout","text":"Successfully deleted directory: /kaggle/working/finetunedModel\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q peft\n!pip install -q accelerate\n!pip install -U bitsandbytes\n!pip install -q transformers\n!pip install -q datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T11:57:05.785617Z","iopub.execute_input":"2025-10-24T11:57:05.786221Z","iopub.status.idle":"2025-10-24T11:58:55.434883Z","shell.execute_reply.started":"2025-10-24T11:57:05.786195Z","shell.execute_reply":"2025-10-24T11:58:55.434121Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm0:02\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCollecting bitsandbytes\n  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.19.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.9.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.3->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.48.1\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install GPUtil","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T11:59:01.830223Z","iopub.execute_input":"2025-10-24T11:59:01.830587Z","iopub.status.idle":"2025-10-24T11:59:07.414258Z","shell.execute_reply.started":"2025-10-24T11:59:01.830559Z","shell.execute_reply":"2025-10-24T11:59:07.413197Z"}},"outputs":[{"name":"stdout","text":"Collecting GPUtil\n  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: GPUtil\n  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=df20af9b2be580b50283c56f017dad50a9792bed9da4676772c036c796b74d91\n  Stored in directory: /root/.cache/pip/wheels/2b/4d/8f/55fb4f7b9b591891e8d3f72977c4ec6c7763b39c19f0861595\nSuccessfully built GPUtil\nInstalling collected packages: GPUtil\nSuccessfully installed GPUtil-1.4.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport GPUtil\nimport os\n\nGPUtil.showUtilization()\n\nif torch.cuda.is_available():\n    print(\"GPU is available\")\nelse:\n    print(\"GPU is not available, using CPU instead\")\n\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T11:59:14.158226Z","iopub.execute_input":"2025-10-24T11:59:14.158586Z","iopub.status.idle":"2025-10-24T11:59:14.465421Z","shell.execute_reply.started":"2025-10-24T11:59:14.158559Z","shell.execute_reply":"2025-10-24T11:59:14.464644Z"}},"outputs":[{"name":"stdout","text":"| ID | GPU | MEM |\n------------------\n|  0 |  0% |  0% |\n|  1 |  0% |  0% |\nGPU is available\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, LlamaTokenizer\nfrom huggingface_hub import notebook_login\n#from datasets import load_dataset\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n\nif \"COLAB_GPU\" in os.environ:\n  from google.colab import output\n  output.enable_custom_widget_manager()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T11:59:17.651572Z","iopub.execute_input":"2025-10-24T11:59:17.652100Z","iopub.status.idle":"2025-10-24T11:59:38.576416Z","shell.execute_reply.started":"2025-10-24T11:59:17.652077Z","shell.execute_reply":"2025-10-24T11:59:38.575709Z"}},"outputs":[{"name":"stderr","text":"2025-10-24 11:59:27.158883: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761307167.340278      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761307167.390456      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"if \"COLAB_GPU\" in os.environ:\n  !huggingface-cli login\nelse:\n  notebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T11:59:43.271061Z","iopub.execute_input":"2025-10-24T11:59:43.271988Z","iopub.status.idle":"2025-10-24T11:59:43.289096Z","shell.execute_reply.started":"2025-10-24T11:59:43.271958Z","shell.execute_reply":"2025-10-24T11:59:43.288101Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba870a29345244c2b9725a2fe1ef6171"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"base_model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config,device_map=\"auto\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T12:00:01.767749Z","iopub.execute_input":"2025-10-24T12:00:01.768397Z","iopub.status.idle":"2025-10-24T12:01:04.178702Z","shell.execute_reply.started":"2025-10-24T12:00:01.768360Z","shell.execute_reply":"2025-10-24T12:01:04.178048Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fcf6a32ae00494db5ac57a22bd1c90a"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df3b999eeac345688f5066dcce4e98c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4343b2435cd546fd9a741a97e192fad6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d7c4e4c3802422eb7b23fe98d664b34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a760121c09dc4b6997d9c6a68bdb01b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b9bb837799548a09580c5bc342640f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a45588da785347ec8213396c989740f1"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"model.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)\n\nconfig = LoraConfig(\n    r=64,\n    lora_alpha=64,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    bias=\"none\",\n    lora_dropout=0.05,\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:05:16.334075Z","iopub.execute_input":"2025-10-15T09:05:16.334730Z","iopub.status.idle":"2025-10-15T09:05:16.933330Z","shell.execute_reply.started":"2025-10-15T09:05:16.334693Z","shell.execute_reply":"2025-10-15T09:05:16.932575Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"if tokenizer.pad_token is None:\n  tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:05:20.137186Z","iopub.execute_input":"2025-10-15T09:05:20.137887Z","iopub.status.idle":"2025-10-15T09:05:20.142954Z","shell.execute_reply.started":"2025-10-15T09:05:20.137852Z","shell.execute_reply":"2025-10-15T09:05:20.142096Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"import pandas as pd\npython_df = pd.read_json(path_or_buf= '/kaggle/input/mbpp-4/mbpp.jsonl', lines=True)\npython_df = python_df[['text', 'code']]\npython_df.rename(columns = {'text': 'question', 'code': 'answer'}, inplace=True)\npython_df.sample(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:05:22.462961Z","iopub.execute_input":"2025-10-15T09:05:22.463567Z","iopub.status.idle":"2025-10-15T09:05:22.556544Z","shell.execute_reply.started":"2025-10-15T09:05:22.463542Z","shell.execute_reply":"2025-10-15T09:05:22.555719Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"                                              question  \\\n165  Write a python function to count the pairs wit...   \n210  Write a python function to count numbers whose...   \n859  Write a function to check whether the given st...   \n516  Write a python function to find the largest po...   \n870  Write a python function to check whether the g...   \n\n                                                answer  \n165  def find_even_Pair(A,N): \\r\\n    evenPair = 0\\...  \n210  def count_Num(n): \\r\\n    if (n == 1): \\r\\n   ...  \n859  import re \\r\\nregex = '[a-zA-z0-9]$'\\r\\ndef ch...  \n516  def largest_pos(list1): \\r\\n    max = list1[0]...  \n870  def are_Rotations(string1,string2): \\r\\n    si...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>165</th>\n      <td>Write a python function to count the pairs wit...</td>\n      <td>def find_even_Pair(A,N): \\r\\n    evenPair = 0\\...</td>\n    </tr>\n    <tr>\n      <th>210</th>\n      <td>Write a python function to count numbers whose...</td>\n      <td>def count_Num(n): \\r\\n    if (n == 1): \\r\\n   ...</td>\n    </tr>\n    <tr>\n      <th>859</th>\n      <td>Write a function to check whether the given st...</td>\n      <td>import re \\r\\nregex = '[a-zA-z0-9]$'\\r\\ndef ch...</td>\n    </tr>\n    <tr>\n      <th>516</th>\n      <td>Write a python function to find the largest po...</td>\n      <td>def largest_pos(list1): \\r\\n    max = list1[0]...</td>\n    </tr>\n    <tr>\n      <th>870</th>\n      <td>Write a python function to check whether the g...</td>\n      <td>def are_Rotations(string1,string2): \\r\\n    si...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"import json\nimport re\nwith open('/kaggle/input/kaggle-data-4/kaggle_data.json','r') as f:\n    kaggle_data = json.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:05:23.880542Z","iopub.execute_input":"2025-10-15T09:05:23.880955Z","iopub.status.idle":"2025-10-15T09:05:23.917265Z","shell.execute_reply.started":"2025-10-15T09:05:23.880933Z","shell.execute_reply":"2025-10-15T09:05:23.916552Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"kaggle_df = pd.DataFrame(kaggle_data)\ndef clean_text(text: str) -> str:\n    text = re.sub(r'<[^>]+>', '', text) # remove HTML/Markdown tags\n    text = re.sub(r'@\\w+', '', text) # remove @user tags\n    text = text.replace('\\n', ' ') # remove newline characters\n    text = re.sub(r'\\s+', ' ', text) # remove multiple spaces\n    text = text.strip() # remove leading and trailing spaces\n    return text\n\nkaggle_df['question'] = kaggle_df['question'].apply(clean_text)\nkaggle_df['answer'] = kaggle_df['answer'].apply(clean_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:05:24.315423Z","iopub.execute_input":"2025-10-15T09:05:24.316164Z","iopub.status.idle":"2025-10-15T09:05:24.367462Z","shell.execute_reply.started":"2025-10-15T09:05:24.316127Z","shell.execute_reply":"2025-10-15T09:05:24.366667Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"data_science_df = pd.read_csv('/kaggle/input/data-sci-1/DataScience QA.csv')\ndata_science_df.rename(columns = {'Question': 'question', 'Answer': 'answer'}, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:05:24.746904Z","iopub.execute_input":"2025-10-15T09:05:24.747519Z","iopub.status.idle":"2025-10-15T09:05:24.766349Z","shell.execute_reply.started":"2025-10-15T09:05:24.747497Z","shell.execute_reply":"2025-10-15T09:05:24.765584Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"kaggle_df = pd.concat([kaggle_df, python_df, data_science_df])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:05:25.138430Z","iopub.execute_input":"2025-10-15T09:05:25.139155Z","iopub.status.idle":"2025-10-15T09:05:25.144662Z","shell.execute_reply.started":"2025-10-15T09:05:25.139120Z","shell.execute_reply":"2025-10-15T09:05:25.143705Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"def text_limit(text: str) -> str:\n    max_len = 1024\n    return text[:max_len] if len(text) > max_len else text\n\nkaggle_df['question'] = kaggle_df['question'].apply(text_limit)\nkaggle_df['answer'] = kaggle_df['answer'].apply(text_limit)\n\nkaggle_df.sample(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:05:25.510737Z","iopub.execute_input":"2025-10-15T09:05:25.511381Z","iopub.status.idle":"2025-10-15T09:05:25.524918Z","shell.execute_reply.started":"2025-10-15T09:05:25.511348Z","shell.execute_reply":"2025-10-15T09:05:25.524090Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"                                              question  \\\n642  Is it possible to install earlier version of t...   \n339  Write a python function to find the sum of the...   \n800  Does Colab stops runtime if it reaches to RAM ...   \n221  Write a function to check if all the elements ...   \n869  Write a function to calculate the sum of the p...   \n\n                                                answer  \n642  Looks like it is impossible. Refer to: https:/...  \n339  def sum_three_smallest_nums(lst):\\r\\n\\treturn ...  \n800  The answer is yes, if you reach the RAM limit,...  \n221  def check_type(test_tuple):\\r\\n  res = True\\r\\...  \n869  def sum_positivenum(nums):\\r\\n  sum_positivenu...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>642</th>\n      <td>Is it possible to install earlier version of t...</td>\n      <td>Looks like it is impossible. Refer to: https:/...</td>\n    </tr>\n    <tr>\n      <th>339</th>\n      <td>Write a python function to find the sum of the...</td>\n      <td>def sum_three_smallest_nums(lst):\\r\\n\\treturn ...</td>\n    </tr>\n    <tr>\n      <th>800</th>\n      <td>Does Colab stops runtime if it reaches to RAM ...</td>\n      <td>The answer is yes, if you reach the RAM limit,...</td>\n    </tr>\n    <tr>\n      <th>221</th>\n      <td>Write a function to check if all the elements ...</td>\n      <td>def check_type(test_tuple):\\r\\n  res = True\\r\\...</td>\n    </tr>\n    <tr>\n      <th>869</th>\n      <td>Write a function to calculate the sum of the p...</td>\n      <td>def sum_positivenum(nums):\\r\\n  sum_positivenu...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"kaggle_data = []\n\nfor _, row in kaggle_df.iterrows():\n    kaggle_data.append(f'Question:\\n{row[\"question\"]}\\n\\Answer:\\n{row[\"answer\"]}')\n\nkaggle_data[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:05:29.855685Z","iopub.execute_input":"2025-10-15T09:05:29.856498Z","iopub.status.idle":"2025-10-15T09:05:29.946708Z","shell.execute_reply.started":"2025-10-15T09:05:29.856465Z","shell.execute_reply":"2025-10-15T09:05:29.945938Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"\"Question:\\nCreate a set from a series in pandas\\n\\\\Answer:\\nIf you only need to get list of unique values, you can just use unique method. If you want to have Python's set, then do set(some_series) In [1]: s = pd.Series([1, 2, 3, 1, 1, 4]) In [2]: s.unique() Out[2]: array([1, 2, 3, 4]) In [3]: set(s) Out[3]: {1, 2, 3, 4} However, if you have DataFrame, just select series out of it ( some_data_frame[''] ).\""},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"tokenized_train_dataset = []\nfor phrase in kaggle_data:\n  tokenized_train_dataset.append(tokenizer(phrase))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:05:32.133744Z","iopub.execute_input":"2025-10-15T09:05:32.134068Z","iopub.status.idle":"2025-10-15T09:05:32.673694Z","shell.execute_reply.started":"2025-10-15T09:05:32.134048Z","shell.execute_reply":"2025-10-15T09:05:32.672960Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"LEARNING_RATE = 5e-5\nWEIGHT_DECAY = 0.01\nBETA_1 = 0.9\nBETA_2 = 0.999\ntrainer = transformers.Trainer(\n    model=model,\n    train_dataset=tokenized_train_dataset,\n    args=transformers.TrainingArguments(\n        output_dir=\"./finetunedModelllama\",\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=2,\n        num_train_epochs=8,\n        learning_rate=LEARNING_RATE,\n        weight_decay=WEIGHT_DECAY,\n        adam_beta1=BETA_1,        \n        adam_beta2=BETA_2,\n        bf16=False,\n        optim=\"paged_adamw_8bit\",\n        logging_dir=\"./log\",\n        save_strategy=\"epoch\",\n        save_steps=50,\n        logging_steps=50,\n        report_to=\"none\"\n\n),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\nmodel.config.use_cache=False\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, glob, shutil\nfrom pathlib import Path\n\nOUT_DIR = Path(\"./finetunedModelllama\")\n\n# list checkpoint folders\nckpts = sorted([p for p in OUT_DIR.glob(\"**/*\") if p.is_dir() and (\"checkpoint\" in p.name or p.name==\"finetunedModel\" or p.name==\"adapter\")])\nprint(\"Found candidate dirs (last few):\")\nfor x in ckpts[-10:]:\n    print(\" \", x)\n\n# heuristics: prefer explicit checkpoint-* folders, else use OUT_DIR itself\ncheckpoint_dirs = sorted([d for d in OUT_DIR.iterdir() if d.is_dir() and (\"checkpoint\" in d.name or d.name==\"adapter\")])\nif len(checkpoint_dirs) > 0:\n    latest_ckpt = checkpoint_dirs[-1]\nelse:\n    latest_ckpt = OUT_DIR  # fallback (Trainer usually writes model files directly here)\n\nprint(\"Using checkpoint folder:\", latest_ckpt)\n\n# Zip the folder into /kaggle/working for download\nzip_name = \"/kaggle/working/finetunedModel_final\"\nshutil.make_archive(zip_name, 'zip', root_dir=str(latest_ckpt))\nprint(\"Zipped to:\", zip_name + \".zip\")\nprint(\"Size (bytes):\", os.path.getsize(zip_name + \".zip\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:39:59.236567Z","iopub.execute_input":"2025-10-15T08:39:59.237005Z","iopub.status.idle":"2025-10-15T08:40:27.824644Z","shell.execute_reply.started":"2025-10-15T08:39:59.236980Z","shell.execute_reply":"2025-10-15T08:40:27.823757Z"}},"outputs":[{"name":"stdout","text":"Found candidate dirs (last few):\n  finetunedModelllama/checkpoint-1467\n  finetunedModelllama/checkpoint-1956\n  finetunedModelllama/checkpoint-2445\n  finetunedModelllama/checkpoint-2934\n  finetunedModelllama/checkpoint-3423\n  finetunedModelllama/checkpoint-3912\n  finetunedModelllama/checkpoint-489\n  finetunedModelllama/checkpoint-978\nUsing checkpoint folder: finetunedModelllama/checkpoint-978\nZipped to: /kaggle/working/finetunedModel_final.zip\nSize (bytes): 536050579\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, LlamaTokenizer\nfrom huggingface_hub import notebook_login\n#from datasets import load_dataset\nfrom peft import PeftModel\n\nif \"COLAB_GPU\" in os.environ:\n  from google.colab import output\n  output.enable_custom_widget_manager()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:40:27.826046Z","iopub.execute_input":"2025-10-15T08:40:27.826347Z","iopub.status.idle":"2025-10-15T08:40:34.898542Z","shell.execute_reply.started":"2025-10-15T08:40:27.826321Z","shell.execute_reply":"2025-10-15T08:40:34.897953Z"}},"outputs":[{"name":"stderr","text":"2025-10-15 08:40:31.848110: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760517631.870305     150 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760517631.877192     150 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"if \"COLAB_GPU\" in os.environ:\n  !huggingface-cli login\nelse:\n  notebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:40:39.295029Z","iopub.execute_input":"2025-10-15T08:40:39.295985Z","iopub.status.idle":"2025-10-15T08:40:39.311790Z","shell.execute_reply.started":"2025-10-15T08:40:39.295958Z","shell.execute_reply":"2025-10-15T08:40:39.310943Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef3deabd83374b1094cd2fc2cca82379"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"!pip uninstall -y bitsandbytes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:40:55.693449Z","iopub.execute_input":"2025-10-15T08:40:55.693742Z","iopub.status.idle":"2025-10-15T08:40:58.932573Z","shell.execute_reply.started":"2025-10-15T08:40:55.693722Z","shell.execute_reply":"2025-10-15T08:40:58.931746Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.48.1)\nRequirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.19.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.9.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.3->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"base_model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config,device_map=\"auto\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T12:04:18.482170Z","iopub.execute_input":"2025-10-24T12:04:18.482944Z","iopub.status.idle":"2025-10-24T12:04:29.187955Z","shell.execute_reply.started":"2025-10-24T12:04:18.482917Z","shell.execute_reply":"2025-10-24T12:04:29.187381Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c720d5da626e43c08c2f45b34ed44271"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True) # 2. Use AutoTokenizer\n\nif tokenizer.pad_token is None:\n  tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T12:05:08.290460Z","iopub.execute_input":"2025-10-24T12:05:08.291012Z","iopub.status.idle":"2025-10-24T12:05:10.021368Z","shell.execute_reply.started":"2025-10-24T12:05:08.290988Z","shell.execute_reply":"2025-10-24T12:05:10.020730Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64d7ab2827614b208dddd0bf65bb2e72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2edcb899991041668e96fb8c2fa1aea0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d39ce74fcf1460e9c564e7cb6686ea8"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from peft import PeftModel\nfine_tuned_model = PeftModel.from_pretrained(model, \"finetunedModelllama/checkpoint-978\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T12:05:44.107900Z","iopub.execute_input":"2025-10-24T12:05:44.108699Z","iopub.status.idle":"2025-10-24T12:05:47.203157Z","shell.execute_reply.started":"2025-10-24T12:05:44.108673Z","shell.execute_reply":"2025-10-24T12:05:47.202284Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def get_dir_size(path='.'):\n    \"\"\"Calculates the size of a directory in megabytes.\"\"\"\n    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            total_size += os.path.getsize(fp)\n    return total_size / (1024 * 1024)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T12:17:23.916052Z","iopub.execute_input":"2025-10-24T12:17:23.916424Z","iopub.status.idle":"2025-10-24T12:17:23.921808Z","shell.execute_reply.started":"2025-10-24T12:17:23.916401Z","shell.execute_reply":"2025-10-24T12:17:23.921017Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import shutil\nimport os\nfrom peft import PeftModel\n\nadapter_path = \"finetunedModelllama/checkpoint-978\"\nmerged_model_save_path = \"/kaggle/working/merged_finetuned_model\"\n\nprint(f\"Loading adapter from: {adapter_path}...\")\n\nfine_tuned_model = PeftModel.from_pretrained(model, adapter_path)\n\nprint(\"Merging adapter into the base model...\")\nmerged_model = fine_tuned_model.merge_and_unload()\nprint(\"Merge complete.\")\n\nprint(f\"Saving merged model to: {merged_model_save_path}...\")\nmerged_model.save_pretrained(merged_model_save_path)\nprint(\"Save complete.\")\n\n# --- 4. Calculate Size on Disk ---\nprint(\"Calculating size of the merged model on disk...\")\n\ntry:\n    disk_size_mb = get_dir_size(merged_model_save_path)\n    print(\"---\" * 10)\n    print(f\"Full Merged Model Size: {disk_size_mb:.2f} MB\")\n    print(\"---\" * 10)\nexcept NameError:\n    print(\"\\n[Error] The 'get_dir_size' function is not defined.\")\n    print(\"Please run the cell that defines 'get_dir_size' and try again.\")\nexcept Exception as e:\n    print(f\"\\nAn error occurred: {e}\")\n\n# --- 5. Clean Up (Important for Kaggle) ---\nprint(f\"Cleaning up by deleting: {merged_model_save_path}\")\nif os.path.exists(merged_model_save_path):\n    shutil.rmtree(merged_model_save_path)\n    print(\"Cleanup complete.\")\nelse:\n    print(\"Cleanup not needed, directory does not exist.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T12:17:49.934615Z","iopub.execute_input":"2025-10-24T12:17:49.935435Z","iopub.status.idle":"2025-10-24T12:18:06.869050Z","shell.execute_reply.started":"2025-10-24T12:17:49.935400Z","shell.execute_reply":"2025-10-24T12:18:06.868149Z"}},"outputs":[{"name":"stdout","text":"Loading adapter from: finetunedModelllama/checkpoint-978...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Merging adapter into the base model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Merge complete.\nSaving merged model to: /kaggle/working/merged_finetuned_model...\nSave complete.\nCalculating size of the merged model on disk...\n------------------------------\nFull Merged Model Size: 2138.87 MB\n------------------------------\nCleaning up by deleting: /kaggle/working/merged_finetuned_model\nCleanup complete.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"def measure_latency(model, tokenizer, query=\"Explain the concept of overfitting. Why is it a problem, and what is one common technique to prevent it?\"):\n    \"\"\"Measures the average latency of the model over several runs.\"\"\"\n    messages = [{\"role\": \"user\", \"content\": query}]\n    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    \n    latencies = []\n    print(\"Running warmup...\")\n    for _ in range(5):\n        _ = model.generate(**inputs, max_new_tokens=50, eos_token_id=tokenizer.eos_token_id)\n    \n    # Timed runs\n    print(\"Running timed benchmark...\")\n    for _ in range(20):\n        start_time = time.perf_counter()\n        _ = model.generate(**inputs, max_new_tokens=50, eos_token_id=tokenizer.eos_token_id)\n        latency = (time.perf_counter() - start_time) * 1000 # Convert to milliseconds\n        latencies.append(latency)\n        \n    avg_latency = np.mean(latencies)\n    std_latency = np.std(latencies)\n    return {\"avg_latency_ms\": avg_latency, \"std_latency_ms\": std_latency}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T12:23:19.766587Z","iopub.execute_input":"2025-10-24T12:23:19.767133Z","iopub.status.idle":"2025-10-24T12:23:19.773010Z","shell.execute_reply.started":"2025-10-24T12:23:19.767111Z","shell.execute_reply":"2025-10-24T12:23:19.772316Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import torch\nimport os\nimport time\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T12:25:33.644403Z","iopub.execute_input":"2025-10-24T12:25:33.644918Z","iopub.status.idle":"2025-10-24T12:25:33.648806Z","shell.execute_reply.started":"2025-10-24T12:25:33.644897Z","shell.execute_reply":"2025-10-24T12:25:33.647885Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"print(\"TEACHER MODEL LATENCY\")\n\nmerged_teacher_latency = measure_latency(merged_model, tokenizer)\n\nprint(f\"MERGED Teacher Latency: {merged_teacher_latency['avg_latency_ms']:.2f} +/- {merged_teacher_latency['std_latency_ms']:.2f} ms\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T12:25:36.418319Z","iopub.execute_input":"2025-10-24T12:25:36.418910Z","iopub.status.idle":"2025-10-24T12:27:23.103574Z","shell.execute_reply.started":"2025-10-24T12:25:36.418884Z","shell.execute_reply":"2025-10-24T12:27:23.102867Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"TEACHER MODEL LATENCY\nRunning warmup...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Running timed benchmark...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"MERGED Teacher Latency: 4245.88 +/- 31.20 ms\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"user_question = \"What is kaggle?\"\n\neval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n\npromptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n\nfine_tuned_model.eval()\n\nwith torch.no_grad():\n  print(tokenizer.decode(fine_tuned_model.generate(**promptTokenized, max_new_tokens=256,repetition_penalty=1.2,eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True))\n  torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:43:55.998730Z","iopub.execute_input":"2025-10-15T08:43:55.999366Z","iopub.status.idle":"2025-10-15T08:44:04.127560Z","shell.execute_reply.started":"2025-10-15T08:43:55.999338Z","shell.execute_reply":"2025-10-15T08:44:04.126842Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question:\nWhat is kaggle?\n\\Just answer this question accurately and concisely.\nKaggle: A platform for data science competitions, hosting datasets and providing tools to users. It's like a social network for data scientists with challenges (kernels) where participants can compete or collaborate on solving problems related to machine learning, programming, and other areas of computer science.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"user_question = \"Write a Python function called find_max that takes a list of numbers as input and returns the largest number in the list.\"\n\neval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n\npromptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n\nfine_tuned_model.eval()\n\nwith torch.no_grad():\n  print(tokenizer.decode(fine_tuned_model.generate(**promptTokenized, max_new_tokens=256,repetition_penalty=1.2,eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True))\n  torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:45:44.746122Z","iopub.execute_input":"2025-10-15T08:45:44.746799Z","iopub.status.idle":"2025-10-15T08:46:16.159413Z","shell.execute_reply.started":"2025-10-15T08:45:44.746774Z","shell.execute_reply":"2025-10-15T08:46:16.158460Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question:\nWrite a Python function called find_max that takes a list of numbers as input and returns the largest number in the list.\n\\Just answer this question accurately and concisely.\ndef find_max(nums):\n    max = nums[0]\n    for x in nums:\n        if x > max:\n            max = x\n    return max  ## This is where you can write your logic. I've written it above already. \n##I just wanted to show how it could be done without using built-in functions like max() or any sort method, because they are not allowed on this site (according to the rules). But we will use them anyway. So instead of writing: def find_max(nums):     max = nums.max()      return max You would simply type: import operator; def find_max(nums):   return max((x)for x in nums);   And it works exactly the same way! So don't worry about what methods are available in python - whatever you need there's probably a method with something similar. The key thing here was understanding the syntax for lists so when someone tries to call an item from a variable (like.list[1]) they'll get confused at first but once you understand that variables store values then calling'myvar[1]' actually gets us back our value stored under myvar[1] which is why you see things like num[nums.index(max)] to take off index of highest val.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"user_question = \"Write a Python function called is_even that takes an integer as input and returns True if the number is even and False otherwise\"\n\neval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n\npromptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n\nfine_tuned_model.eval()\n\nwith torch.no_grad():\n  print(tokenizer.decode(fine_tuned_model.generate(**promptTokenized, max_new_tokens=256,repetition_penalty=1.2,eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True))\n  torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:47:26.443372Z","iopub.execute_input":"2025-10-15T08:47:26.443689Z","iopub.status.idle":"2025-10-15T08:47:57.717394Z","shell.execute_reply.started":"2025-10-15T08:47:26.443667Z","shell.execute_reply":"2025-10-15T08:47:57.716703Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question:\nWrite a Python function called is_even that takes an integer as input and returns True if the number is even and False otherwise\n\\Just answer this question accurately and concisely.\ndef is_even(n): return n % 2 ==0  ##answer:Python function to check whether given no. is odd or not##def is_odd(x) : return x % 2!= 0  ##answer:Pascal Triangle Formula##import math ##def get_sum(m,n) : return (m * (1 + m)) // 2 + (n - m) * ((m + 1) * (3 * m + 1) // 6) ##answer:Sieve of Eratosthenes Algorithm##def prime_sieve(limit) : primes = [] for num in range(2, limit+1) : flag = True for i in range(2,int(math.sqrt(num))) : if num % i == 0 : flag = False break if flag : primes.append(num) return primes ##answer:Fibonacci Series##def fibonacci_series(n) : fib_series = [0] * (n+1) for i in range(1,n+1) : fib_series[i] = fib_series[i-1] + fib_series[i-2] return fib_series ##answer:Factorial Function##def factorial_number(n) : fact_num = 1 for i in range(1,n+1) : fact\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"user_question = \"What is the difference between classification and regression in machine learning? Can you give an example of each?\"\n\neval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n\npromptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n\nfine_tuned_model.eval()\n\nwith torch.no_grad():\n  print(tokenizer.decode(fine_tuned_model.generate(**promptTokenized, max_new_tokens=256,repetition_penalty=1.2,eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True))\n  torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:49:02.391899Z","iopub.execute_input":"2025-10-15T08:49:02.392621Z","iopub.status.idle":"2025-10-15T08:49:33.607154Z","shell.execute_reply.started":"2025-10-15T08:49:02.392598Z","shell.execute_reply":"2025-10-15T08:49:33.606433Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question:\nWhat is the difference between classification and regression in machine learning? Can you give an example of each?\n\\Just answer this question accurately and concisely.\nAnswer:\nClassification: predicts categorical labels, e.g., spam vs. not spam email; Regression: predicts continuous values, e.g., house price based on features like number of bedrooms. Example: Predicting whether a customer will buy a car (0/1) or predicting how many houses to build given certain characteristics (house price). In both cases we have a target variable that can take multiple possible outcomes. However they differ from one another because classification problems are often discrete while regression problems are continuous. Also note that linear regression isn't strictly a regression problem since it's binary but I used your wording for consistency. So if someone asks about what type of algorithm is best suited for prediction then I would say linear regression without hesitation even though technically its only partially correct. Next step would be discussing some common metrics for evaluating such models like accuracy score which gives percentage-wise true predictions out of all total predictions made by model whereas mean squared error provides average squared differences between predicted and actual value. Both these metrics help us understand performance of our trained model and guide further tuning. I also think there might be slight confusion regarding term'regression'. Generally speaking regression implies estimating the relationship between independent variables with dependent variable being numerical so saying linear regression means using linear equations to relate them. There is no\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"user_question = \"Explain the concept of overfitting. Why is it a problem, and what is one common technique to prevent it?\"\n\neval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n\npromptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n\nfine_tuned_model.eval()\n\nwith torch.no_grad():\n  print(tokenizer.decode(fine_tuned_model.generate(**promptTokenized, max_new_tokens=100,repetition_penalty=1.2,eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True))\n  torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:49:52.674467Z","iopub.execute_input":"2025-10-15T08:49:52.674752Z","iopub.status.idle":"2025-10-15T08:50:05.097259Z","shell.execute_reply.started":"2025-10-15T08:49:52.674735Z","shell.execute_reply":"2025-10-15T08:50:05.096529Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question:\nExplain the concept of overfitting. Why is it a problem, and what is one common technique to prevent it?\n\\Just answer this question accurately and concisely.\nOverfitting occurs when a model learns too closely from training data such that its performance on unseen test data degrades significantly. This happens if the number of parameters in your model exceeds the amount of information available for learning by far. The best way to deal with overfitting is regularization which involves adding some penalty term into loss function so that smaller models are favored. Commonly used regularizers include L1 & L2 penalties. For instance, Ridge regression uses L2 penalty while Lasso uses L\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"user_question = \"What is a p-value, and what does it typically signify in the context of hypothesis testing?\"\n\neval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n\npromptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n\nfine_tuned_model.eval()\n\nwith torch.no_grad():\n  print(tokenizer.decode(fine_tuned_model.generate(**promptTokenized, max_new_tokens=256,repetition_penalty=1.2,eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True))\n  torch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"user_question = \"how can i become a grandmaster in kaggle?\"\n\neval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n\npromptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n\nfine_tuned_model.eval()\n\nwith torch.no_grad():\n  print(tokenizer.decode(fine_tuned_model.generate(**promptTokenized, max_new_tokens=256,repetition_penalty=1.2,eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True))\n  torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:51:40.978923Z","iopub.execute_input":"2025-10-15T08:51:40.979579Z","iopub.status.idle":"2025-10-15T08:52:12.210453Z","shell.execute_reply.started":"2025-10-15T08:51:40.979554Z","shell.execute_reply":"2025-10-15T08:52:12.209590Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question:\nhow can i become a grandmaster in kaggle?\n\\Just answer this question accurately and concisely.\nI am not sure what you mean by \"beyond\" or how to achieve that, but I will give some advice. To compete with the top teams on Kaggle is hard work! First of all, practice your skills thoroughly so as to be proficient at them. It's always good for you to know about different algorithms used in machine learning competitions. Familiarize yourself with the common techniques such as decision trees, random forest, support vector machines (SVM), neural networks, etc., and learn their respective pros & cons. For example: Decision Trees are simple yet efficient, they split data into two subsets based upon an independent feature. Random Forests improve over individual models through ensemble learning. SVM works well when dealing with high dimensional space. Neural Networks have been widely used due to its ability to learn complex patterns easily. Secondly, read other people’s solutions and understand why it was working/why it wasn’t. You may get ideas from there too. Lastly, do participate actively in competitions like those mentioned above. This way you’ll gain experience and build confidence which would help you in future endeavors. Remember, success comes after failure. Don't lose hope if you don't win immediately. Good luck! :) PS : There isn't any direct path to becoming a Grand\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"DISTILLATION","metadata":{}},{"cell_type":"code","source":"base_model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\ntecher_model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config,device_map=\"auto\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:55:32.240135Z","iopub.execute_input":"2025-10-15T08:55:32.240702Z","iopub.status.idle":"2025-10-15T08:55:54.263240Z","shell.execute_reply.started":"2025-10-15T08:55:32.240676Z","shell.execute_reply":"2025-10-15T08:55:54.262630Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fab161677ca4271a07fa16ecc7d123e"}},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True) # 2. Use AutoTokenizer\n\nif tokenizer.pad_token is None:\n  tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:56:15.995216Z","iopub.execute_input":"2025-10-15T08:56:15.995834Z","iopub.status.idle":"2025-10-15T08:56:16.556972Z","shell.execute_reply.started":"2025-10-15T08:56:15.995795Z","shell.execute_reply":"2025-10-15T08:56:16.556360Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from peft import PeftModel\nteacher_model = PeftModel.from_pretrained(techer_model,\"finetunedModelllama/checkpoint-978\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:57:02.073509Z","iopub.execute_input":"2025-10-15T08:57:02.073857Z","iopub.status.idle":"2025-10-15T08:57:03.574106Z","shell.execute_reply.started":"2025-10-15T08:57:02.073826Z","shell.execute_reply":"2025-10-15T08:57:03.573410Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"for param in teacher_model.parameters():\n    param.requires_grad = False\nteacher_model.eval()\nprint(\"Teacher model loaded and frozen.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:57:08.974228Z","iopub.execute_input":"2025-10-15T08:57:08.974513Z","iopub.status.idle":"2025-10-15T08:57:08.991149Z","shell.execute_reply.started":"2025-10-15T08:57:08.974492Z","shell.execute_reply":"2025-10-15T08:57:08.990376Z"}},"outputs":[{"name":"stdout","text":"Teacher model loaded and frozen.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"student_model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\nstudent_model = AutoModelForCausalLM.from_pretrained(student_model_id, quantization_config=bnb_config,device_map=\"auto\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:57:34.209248Z","iopub.execute_input":"2025-10-15T08:57:34.209967Z","iopub.status.idle":"2025-10-15T08:57:38.120410Z","shell.execute_reply.started":"2025-10-15T08:57:34.209935Z","shell.execute_reply":"2025-10-15T08:57:38.119600Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"from peft import get_peft_model,LoraConfig,prepare_model_for_kbit_training","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:58:49.519461Z","iopub.execute_input":"2025-10-15T08:58:49.520157Z","iopub.status.idle":"2025-10-15T08:58:49.524255Z","shell.execute_reply.started":"2025-10-15T08:58:49.520125Z","shell.execute_reply":"2025-10-15T08:58:49.523355Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"student_model.gradient_checkpointing_enable()\nstudent_model = prepare_model_for_kbit_training(student_model)\n\nconfig = LoraConfig(\n    r=64,\n    lora_alpha=64,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    bias=\"none\",\n    lora_dropout=0.05,\n    task_type=\"CAUSAL_LM\"\n)\nstudent_model = get_peft_model(student_model, config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:58:51.819368Z","iopub.execute_input":"2025-10-15T08:58:51.819647Z","iopub.status.idle":"2025-10-15T08:58:52.423838Z","shell.execute_reply.started":"2025-10-15T08:58:51.819623Z","shell.execute_reply":"2025-10-15T08:58:52.422902Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Set the environment variable to reduce fragmentation\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:18:11.596384Z","iopub.execute_input":"2025-10-15T09:18:11.596958Z","iopub.status.idle":"2025-10-15T09:18:11.600649Z","shell.execute_reply.started":"2025-10-15T09:18:11.596931Z","shell.execute_reply":"2025-10-15T09:18:11.599915Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n\nclass DistillationTrainer(Trainer):\n    def __init__(self, *args, teacher_model=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.teacher_model = teacher_model\n\n    # --- The signature of this method is the only part that changed ---\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        # Forward pass for the student model (now using `model`)\n        outputs_stu = model(**inputs)\n        logits_stu = outputs_stu.logits\n\n        # Forward pass for the teacher model\n        with torch.no_grad():\n            outputs_tea = self.teacher_model(**inputs)\n            logits_tea = outputs_tea.logits\n\n        # Define the loss function\n        loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n\n        # Calculate the distillation loss, scaled by T^2\n        loss_kd = self.args.temperature ** 2 * loss_fct(\n            F.log_softmax(logits_stu / self.args.temperature, dim=-1),\n            F.softmax(logits_tea / self.args.temperature, dim=-1)\n        )\n\n        return (loss_kd, outputs_stu) if return_outputs else loss_kd\n\n# --- 2. Custom Training Arguments to include Temperature ---\nclass DistillationTrainingArguments(TrainingArguments):\n    def __init__(self, *args, temperature=2.0, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.temperature = temperature\n\n# --- 3. Define the Training Configuration ---\ntraining_args = DistillationTrainingArguments(\n    output_dir=\"./distilled_model_checkpoints\",\n    num_train_epochs=3,                 # Recommended starting point\n    per_device_train_batch_size=1,      # Adjust if you face memory issues\n    gradient_accumulation_steps=16,\n    fp16=True,# Creates an effective batch size of 16\n    learning_rate=5e-5,\n    save_strategy=\"epoch\",\n    logging_steps=50,\n    temperature=2.0,                    # The distillation hyperparameter\n    report_to=\"none\",\n)\n\n# --- 4. Define the Data Collator ---\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, # Your shared tokenizer\n    mlm=False\n)\n\n# --- 5. Initialize the Final Trainer ---\ndistillation_trainer = DistillationTrainer(\n    model=student_model,                # Your trainable student model\n    teacher_model=teacher_model,        # Your frozen teacher model\n    args=training_args,\n    train_dataset=tokenized_train_dataset,    # Your prepared dataset\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)\n\nprint(\"✅ DistillationTrainer is configured and ready for training.\")\ndistillation_trainer.train()\nprint(\"✅ Distillation training has completed successfully!\")\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:21:15.466553Z","iopub.execute_input":"2025-10-15T09:21:15.466939Z","iopub.status.idle":"2025-10-15T10:36:02.041028Z","shell.execute_reply.started":"2025-10-15T09:21:15.466914Z","shell.execute_reply":"2025-10-15T10:36:02.040313Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_150/1249504526.py:7: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `DistillationTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(*args, **kwargs)\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"✅ DistillationTrainer is configured and ready for training.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='369' max='369' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [369/369 1:14:33, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>2957.983700</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2031.663400</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1807.269500</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1604.313000</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1575.109500</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1553.209100</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1582.962800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"✅ Distillation training has completed successfully!\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"import os, glob, shutil\nfrom pathlib import Path\n\nOUT_DIR = Path(\"./distilled_model_checkpoints\")\n\n# list checkpoint folders\nckpts = sorted([p for p in OUT_DIR.glob(\"**/*\") if p.is_dir() and (\"checkpoint\" in p.name or p.name==\"finetunedModel\" or p.name==\"adapter\")])\nprint(\"Found candidate dirs (last few):\")\nfor x in ckpts[-10:]:\n    print(\" \", x)\n\n# heuristics: prefer explicit checkpoint-* folders, else use OUT_DIR itself\ncheckpoint_dirs = sorted([d for d in OUT_DIR.iterdir() if d.is_dir() and (\"checkpoint\" in d.name or d.name==\"adapter\")])\nif len(checkpoint_dirs) > 0:\n    latest_ckpt = checkpoint_dirs[-1]\nelse:\n    latest_ckpt = OUT_DIR  # fallback (Trainer usually writes model files directly here)\n\nprint(\"Using checkpoint folder:\", latest_ckpt)\n\n# Zip the folder into /kaggle/working for download\nzip_name = \"/kaggle/working/distilled_model_final\"\nshutil.make_archive(zip_name, 'zip', root_dir=str(latest_ckpt))\nprint(\"Zipped to:\", zip_name + \".zip\")\nprint(\"Size (bytes):\", os.path.getsize(zip_name + \".zip\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T10:38:49.816070Z","iopub.execute_input":"2025-10-15T10:38:49.816402Z","iopub.status.idle":"2025-10-15T10:39:17.236493Z","shell.execute_reply.started":"2025-10-15T10:38:49.816378Z","shell.execute_reply":"2025-10-15T10:39:17.235361Z"}},"outputs":[{"name":"stdout","text":"Found candidate dirs (last few):\n  distilled_model_checkpoints/checkpoint-123\n  distilled_model_checkpoints/checkpoint-246\n  distilled_model_checkpoints/checkpoint-369\nUsing checkpoint folder: distilled_model_checkpoints/checkpoint-369\nZipped to: /kaggle/working/distilled_model_final.zip\nSize (bytes): 499159528\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"distilled_model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\ndistilled_model = AutoModelForCausalLM.from_pretrained(distilled_model_id, quantization_config=bnb_config,device_map=\"auto\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T10:42:42.268570Z","iopub.execute_input":"2025-10-15T10:42:42.268987Z","iopub.status.idle":"2025-10-15T10:42:45.719074Z","shell.execute_reply.started":"2025-10-15T10:42:42.268964Z","shell.execute_reply":"2025-10-15T10:42:45.718232Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(distilled_model_id, trust_remote_code=True) # 2. Use AutoTokenizer\n\nif tokenizer.pad_token is None:\n  tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\nfrom peft import PeftModel\ndistilled_model = PeftModel.from_pretrained(distilled_model,\"distilled_model_checkpoints/checkpoint-369\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T10:43:20.558775Z","iopub.execute_input":"2025-10-15T10:43:20.559549Z","iopub.status.idle":"2025-10-15T10:43:21.880537Z","shell.execute_reply.started":"2025-10-15T10:43:20.559524Z","shell.execute_reply":"2025-10-15T10:43:21.879926Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"user_question = \"how can i become a grandmaster in kaggle?\"\n\neval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n\npromptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n\ndistilled_model.eval()\n\nwith torch.no_grad():\n  print(tokenizer.decode(distilled_model.generate(**promptTokenized, max_new_tokens=256,repetition_penalty=1.2,eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True))\n  torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T10:51:24.802512Z","iopub.execute_input":"2025-10-15T10:51:24.802859Z","iopub.status.idle":"2025-10-15T10:51:41.084662Z","shell.execute_reply.started":"2025-10-15T10:51:24.802836Z","shell.execute_reply":"2025-10-15T10:51:41.083856Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question:\nhow can i become a grandmaster in kaggle?\n\\Just answer this question accurately and concisely.\nAnswer:\nTo be considered for the Grand Master title on Kaggle, you must have at least 2.5 million points (or more) with an average score of at least 0.4 or higher per competition. To achieve that level of achievement requires significant effort and dedication to compete consistently against top-level competitors. The current leader is Tom Bortolano who has achieved it by winning multiple competitions with high scores. However, note that achieving the Grand Master title does not guarantee entry into the KAGGLE community, as there are many other ways to contribute and participate, such as contributing code, providing feedback, or participating in discussions. So don't get discouraged if your goal isn't immediately achievable; keep working towards it! Good luck! [Note: This response was written based on information available up until my knowledge cutoff date (01 March 2023). If any changes occur after then, please let me know.] - Note: As mentioned above, I'm just answering the question directly without further explanation or context. For additional guidance or specific advice related to competing for the Grand Master title on Kaggle, refer to official resources and guidelines provided by the platform.\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"user_question = \"Write a Python function called find_max that takes a list of numbers as input and returns the largest number in the list.\"\n\neval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n\npromptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n\ndistilled_model.eval()\n\nwith torch.no_grad():\n  print(tokenizer.decode(distilled_model.generate(**promptTokenized, max_new_tokens=256,repetition_penalty=1.2,eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True))\n  torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T10:59:14.852553Z","iopub.execute_input":"2025-10-15T10:59:14.852927Z","iopub.status.idle":"2025-10-15T10:59:32.525105Z","shell.execute_reply.started":"2025-10-15T10:59:14.852905Z","shell.execute_reply":"2025-10-15T10:59:32.524366Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question:\nWrite a Python function called find_max that takes a list of numbers as input and returns the largest number in the list.\n\\Just answer this question accurately and concisely.\n\\Answer:\ndef find_max(x): \n    return max(x)  # Returns the maximum element in the given list.   ``` ``` ## Example Use Case: x = [1,2,3,4,5] print(find_max(x))  # Output: 5 ``` ``` ## Error Handling: x = [-1,-1,-1] try: y = find_max(x) except ValueError: print(\"No valid value found\")  # No error occurs because there is no valid value for -1-1-1. If you want to handle it differently (e.g., raise an exception), you can add a catch block like this: try: y = find_max(x) except ValueError: raise ValueError('Invalid Value')  # Raises a ValueError if no valid value exists in the list. In this case, we could either ignore the invalid values or throw them away when finding the maximum. The best solution depends on your specific use case. For example: def find_max(x): return max([x for x in x if x!= None])  # Finds the first non-null value from each item in the list. This will only work with lists where all items are not null. If you have a list with some null elements, then you'll\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"user_question = \"Write a Python function called is_even that takes an integer as input and returns True if the number is even and False otherwise.\"\n\neval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n\npromptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n\ndistilled_model.eval()\n\nwith torch.no_grad():\n  print(tokenizer.decode(distilled_model.generate(**promptTokenized, max_new_tokens=256,repetition_penalty=1.2,eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True))\n  torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T11:03:32.532456Z","iopub.execute_input":"2025-10-15T11:03:32.532779Z","iopub.status.idle":"2025-10-15T11:03:50.174139Z","shell.execute_reply.started":"2025-10-15T11:03:32.532759Z","shell.execute_reply":"2025-10-15T11:03:50.173185Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question:\nWrite a Python function called is_even that takes an integer as input and returns True if the number is even and False otherwise.\n\\Just answer this question accurately and concisely.\n\\Answer:\ndef is_even(n): \n    return n % 2 == 0;  # Returns true if n is even, false else.  # Using lambda function for simplicity.  def is_even(n): return (lambda x: x % 2 == 0) (n)  # The above code can be written in one line using lambda function.  # Example use case: print(is_even(10))  # Output: True  # Example use case: print(is_even(-1))  # Output: False  # Example use case: print(is_even(3))  # Output: True  # Example use case: print(is_even(4))  # Output: True  # Example use case: print(is_even(5))  # Output: False  # Example use case: print(is_even(6))  # Output: True  # Example use case: print(is_even(7))  # Output: False  # Example use case: print(is_even(8))  # Output: True  # Example use case: print(is_even(9))  # Output: False  # Example use case: print(is_even(10))  # Output: True  # Example use case: print(is_even(\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"user_question = \"what is the difference between the classification and regression in machine learning?\"\n\neval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n\npromptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n\ndistilled_model.eval()\n\nwith torch.no_grad():\n  print(tokenizer.decode(distilled_model.generate(**promptTokenized, max_new_tokens=256,repetition_penalty=1.2,eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True))\n  torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T11:07:18.134726Z","iopub.execute_input":"2025-10-15T11:07:18.135507Z","iopub.status.idle":"2025-10-15T11:07:35.698029Z","shell.execute_reply.started":"2025-10-15T11:07:18.135483Z","shell.execute_reply":"2025-10-15T11:07:35.697190Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question:\nwhat is the difference between the classification and regression in machine learning?\n\\Just answer this question accurately and concisely.\n\\Answer:\nClassification: Predicting a categorical outcome (e.g., 0/1, yes/no) based on input features. Regression: Predicting a continuous output value (e.g., price of an item) based on input features. Both are used to train models for prediction tasks, but they have different objectives and requirements. Classification aims to predict one specific class or category from a set of possible classes. Regressions aim to predict a continuous range of values within a given space. Different types of regression can be found, such as linear regression and logistic regression. The choice depends on the type of problem you're trying to solve and your data's structure. In most cases, the task will involve some form of classification. So I'll give a simple example. Let's say we want to find out whether someone is likely to buy a car or not. If it were a regression task, then our model would try to predict how much money the person might spend if he bought a car. However, that is quite complex and difficult to understand without knowing more about the dataset. A simpler way to look at it would be like predicting what color car you'd get if you had $1000000. That is a very easy task! Therefore my final answer is:\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"user_question = \"Explain the concept of overfitting. Why is it a problem, and what is one common technique to prevent it?\"\n\neval_prompt = f'Question:\\n{user_question}\\n\\Just answer this question accurately and concisely.\\n'\n\npromptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n\ndistilled_model.eval()\n\nwith torch.no_grad():\n  print(tokenizer.decode(distilled_model.generate(**promptTokenized, max_new_tokens=256,repetition_penalty=1.2,eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True))\n  torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T11:11:09.987987Z","iopub.execute_input":"2025-10-15T11:11:09.988332Z","iopub.status.idle":"2025-10-15T11:11:27.599312Z","shell.execute_reply.started":"2025-10-15T11:11:09.988309Z","shell.execute_reply":"2025-10-15T11:11:27.598588Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question:\nExplain the concept of overfitting. Why is it a problem, and what is one common technique to prevent it?\n\\Just answer this question accurately and concisely.\n\\Answer:\nOverfitting occurs when a model fits too well to the training data, resulting in poor performance on new or unseen data. It's a problem because models that are overly complex can be difficult to train and may not generalize well to other situations. One common technique to prevent overfitting is regularization: adding penalties to the loss function (e.g., L1 or L2 regularization) encourages the model to learn more general patterns rather than just fitting the specific details of the training data. This helps reduce overfitting by preventing the model from becoming too specialized. Example: Regularized logistic regression with L2 regularizer: log(1 + e^(-x)) = 1 - x + L2(x) where L2(x) = 1 / 2 * (1 + e^(-x)^2). The goal here is to penalize large values of x, which makes the model less sensitive to small changes in input. In contrast, unregularized logistic regression has no penalty term, making it harder for the model to avoid overfitting. By using L2 regularization, we're encouraging the model to fit the underlying pattern without getting stuck in local minima. This reduces overfitting while still keeping some flexibility to adapt to different inputs\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"import shutil\nimport os\nimport torch\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel\n\n# --- 1. Define Paths & Config ---\n# Make sure your 'get_dir_size' function is already defined!\nstudent_base_model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\nstudent_adapter_path = \"distilled_model_checkpoints/checkpoint-369\"\nmerged_student_save_path = \"/kaggle/working/merged_distilled_model\"\n\n# We must load the base model in the *same config* as training\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# --- 2. Load the Base Student Model ---\nprint(f\"Loading base student model: {student_base_model_id}...\")\n# We load it fresh to ensure there's no conflict\nbase_student_model = AutoModelForCausalLM.from_pretrained(\n    student_base_model_id,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\n\n# --- 3. Load the Distilled Adapter ---\nprint(f\"Loading student adapter from: {student_adapter_path}...\")\ndistilled_peft_model = PeftModel.from_pretrained(base_student_model, student_adapter_path)\n\n# --- 4. Merge and Unload ---\nprint(\"Merging student adapter into the base model...\")\nmerged_student_model = distilled_peft_model.merge_and_unload()\nprint(\"Merge complete.\")\n\n# --- 5. Save the Merged Model to Disk ---\nprint(f\"Saving merged student model to: {merged_student_save_path}...\")\nmerged_student_model.save_pretrained(merged_student_save_path)\nprint(\"Save complete.\")\n\n# --- 6. Calculate Size on Disk ---\nprint(\"Calculating size of the merged student model on disk...\")\ntry:\n    student_disk_size_mb = get_dir_size(merged_student_save_path)\n    print(\"---\" * 10)\n    print(f\"Full Merged DISTILLED STUDENT Model Size: {student_disk_size_mb:.2f} MB\")\n    print(\"---\" * 10)\nexcept NameError:\n    print(\"\\n[Error] The 'get_dir_size' function is not defined.\")\n    print(\"Please run the cell that defines 'get_dir_size' and try again.\")\nexcept Exception as e:\n    print(f\"\\nAn error occurred: {e}\")\n\n# --- 7. Clean Up (Important for Kaggle) ---\nprint(f\"Cleaning up by deleting: {merged_student_save_path}\")\nif os.path.exists(merged_student_save_path):\n    shutil.rmtree(merged_student_save_path)\n    print(\"Cleanup complete.\")\nelse:\n    print(\"Cleanup not needed, directory does not exist.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T12:31:08.892175Z","iopub.execute_input":"2025-10-24T12:31:08.893079Z","iopub.status.idle":"2025-10-24T12:31:30.675702Z","shell.execute_reply.started":"2025-10-24T12:31:08.893049Z","shell.execute_reply":"2025-10-24T12:31:30.674772Z"}},"outputs":[{"name":"stdout","text":"Loading base student model: meta-llama/Llama-3.2-1B-Instruct...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d8bc69ea29641acb363938aeffe1ab6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6adca125797342cda4421ba65a4fdeb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4436f30234d4e1aa53b10d52e3974ae"}},"metadata":{}},{"name":"stdout","text":"Loading student adapter from: distilled_model_checkpoints/checkpoint-369...\nMerging student adapter into the base model...\nMerge complete.\nSaving merged student model to: /kaggle/working/merged_distilled_model...\nSave complete.\nCalculating size of the merged student model on disk...\n------------------------------\nFull Merged DISTILLED STUDENT Model Size: 980.07 MB\n------------------------------\nCleaning up by deleting: /kaggle/working/merged_distilled_model\nCleanup complete.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"print(\"STUDENT MODEL LATENCY\")\nmerged_student_latency = measure_latency(merged_student_model, tokenizer)\nprint(f\"MERGED Student Latency: {merged_student_latency['avg_latency_ms']:.2f} +/- {merged_student_latency['std_latency_ms']:.2f} ms\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T12:36:46.519599Z","iopub.execute_input":"2025-10-24T12:36:46.519899Z","iopub.status.idle":"2025-10-24T12:37:44.229164Z","shell.execute_reply.started":"2025-10-24T12:36:46.519878Z","shell.execute_reply":"2025-10-24T12:37:44.228410Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"STUDENT MODEL LATENCY\nRunning warmup...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Running timed benchmark...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"MERGED Student Latency: 2304.71 +/- 36.27 ms\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"MERGED Teacher Latency: 4245.88 +/- 31.20 ms\nMERGED Student Latency: 2304.71 +/- 36.27 ms\n\nFull Merged Model Size: 2138.87 MB \nFull Merged DISTILLED STUDENT Model Size: 980.07 MB","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}